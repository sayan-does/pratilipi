{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting implicit\n",
      "  Downloading implicit-0.7.2-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.26.2)\n",
      "Requirement already satisfied: scipy>=0.16 in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from implicit) (1.15.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from implicit) (4.66.1)\n",
      "Requirement already satisfied: threadpoolctl in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from implicit) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sayan\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sayan\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->implicit) (0.4.4)\n",
      "Downloading implicit-0.7.2-cp310-cp310-win_amd64.whl (748 kB)\n",
      "   ---------------------------------------- 0.0/748.6 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 262.1/748.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 748.6/748.6 kB 2.1 MB/s eta 0:00:00\n",
      "Installing collected packages: implicit\n",
      "Successfully installed implicit-0.7.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "%pip install implicit scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(user_interactions_path, meta_data_path):\n",
    "    interactions_df = pd.read_csv(user_interactions_path)\n",
    "    meta_df = pd.read_csv(meta_data_path)\n",
    "    \n",
    "    # Convert timestamps\n",
    "    interactions_df['updated_at'] = pd.to_datetime(interactions_df['updated_at'])\n",
    "    meta_df['updated_at'] = pd.to_datetime(meta_df['updated_at'])\n",
    "    meta_df['published_at'] = pd.to_datetime(meta_df['published_at'])\n",
    "    \n",
    "    interactions_df = interactions_df.sort_values('updated_at')\n",
    "    \n",
    "    return interactions_df, meta_df\n",
    "\n",
    "interactions_df, meta_df = load_and_preprocess_data('user_interaction.csv', 'metadata.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Interactions:\n",
      "                  user_id      pratilipi_id  read_percent  \\\n",
      "1033131  5506791954036110  1377786225804654         100.0   \n",
      "1415300  5506791980439899  1377786228150074         100.0   \n",
      "2318259  5506791979182708  1377786218415632         100.0   \n",
      "952322   5506791996330389  1377786219497547         100.0   \n",
      "2114134  5506791961370166  1377786224952303         100.0   \n",
      "\n",
      "                     updated_at  \n",
      "1033131 2022-03-18 15:14:41.827  \n",
      "1415300 2022-03-18 15:14:42.120  \n",
      "2318259 2022-03-18 15:14:42.134  \n",
      "952322  2022-03-18 15:14:42.170  \n",
      "2114134 2022-03-18 15:14:42.282  \n",
      "\n",
      "Meta Data:\n",
      "          author_id      pratilipi_id category_name  reading_time  \\\n",
      "0 -3418949279741297  1025741862639304   translation             0   \n",
      "1 -2270332351871840  1377786215601277   translation           171   \n",
      "2 -2270332352037261  1377786215601962   translation            92   \n",
      "3 -2270332352521845  1377786215640994   translation             0   \n",
      "4 -2270332349665658  1377786215931338   translation            47   \n",
      "\n",
      "           updated_at        published_at  \n",
      "0 2020-08-19 15:26:13 2016-09-30 10:37:04  \n",
      "1 2021-01-21 16:27:07 2018-06-11 13:17:48  \n",
      "2 2020-09-29 12:33:57 2018-06-12 04:19:12  \n",
      "3 2019-10-17 09:03:37 2019-09-26 14:58:53  \n",
      "4 2020-05-05 11:33:41 2018-11-25 12:28:23  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"User Interactions:\")\n",
    "print(interactions_df.head())\n",
    "print(\"\\nMeta Data:\")\n",
    "print(meta_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data structure:\n",
      "\n",
      "Columns: ['user_id', 'pratilipi_id', 'read_percent', 'updated_at']\n",
      "\n",
      "Sample of data:\n",
      "                  user_id      pratilipi_id  read_percent  \\\n",
      "1033131  5506791954036110  1377786225804654         100.0   \n",
      "1415300  5506791980439899  1377786228150074         100.0   \n",
      "2318259  5506791979182708  1377786218415632         100.0   \n",
      "952322   5506791996330389  1377786219497547         100.0   \n",
      "2114134  5506791961370166  1377786224952303         100.0   \n",
      "\n",
      "                     updated_at  \n",
      "1033131 2022-03-18 15:14:41.827  \n",
      "1415300 2022-03-18 15:14:42.120  \n",
      "2318259 2022-03-18 15:14:42.134  \n",
      "952322  2022-03-18 15:14:42.170  \n",
      "2114134 2022-03-18 15:14:42.282  \n",
      "\n",
      "Data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2500000 entries, 1033131 to 2186369\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Dtype         \n",
      "---  ------        -----         \n",
      " 0   user_id       int64         \n",
      " 1   pratilipi_id  int64         \n",
      " 2   read_percent  float64       \n",
      " 3   updated_at    datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(1), int64(2)\n",
      "memory usage: 95.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking data structure:\")\n",
    "print(\"\\nColumns:\", interactions_df.columns.tolist())\n",
    "print(\"\\nSample of data:\")\n",
    "print(interactions_df.head())\n",
    "print(\"\\nData info:\")\n",
    "print(interactions_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "user_mapping = {id: i for i, id in enumerate(\n",
    "    interactions_df['user_id'].unique())}\n",
    "item_mapping = {id: i for i, id in enumerate(\n",
    "    interactions_df['pratilipi_id'].unique())}\n",
    "\n",
    "# 2. Create the necessary variables for sparse matrix\n",
    "user_indices = [user_mapping[user] for user in interactions_df['user_id']]\n",
    "item_indices = [item_mapping[item] for item in interactions_df['pratilipi_id']]\n",
    "# Using read_percent as ratings\n",
    "ratings = interactions_df['read_percent'].values\n",
    "\n",
    "# 3. Get dimensions for the matrix\n",
    "n_users = len(user_mapping)\n",
    "n_items = len(item_mapping)\n",
    "\n",
    "# 4. Now create the sparse matrix\n",
    "sparse_matrix = csr_matrix((ratings, (user_indices, item_indices)),\n",
    "                           shape=(n_users, n_items),\n",
    "                           dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def split_train_test(sparse_matrix, test_size=0.25, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the sparse interaction matrix into training and test sets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sparse_matrix : scipy.sparse.csr_matrix\n",
    "        The user-item interaction matrix\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing (default: 0.25 as per README)\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    train_matrix : scipy.sparse.csr_matrix\n",
    "        Training set matrix\n",
    "    test_matrix : scipy.sparse.csr_matrix\n",
    "        Test set matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # Get indices of all non-zero elements\n",
    "    nonzero = sparse_matrix.nonzero()\n",
    "    indices = list(zip(nonzero[0], nonzero[1]))\n",
    "    values = sparse_matrix.data\n",
    "\n",
    "    # Split the indices\n",
    "    train_indices, test_indices = train_test_split(\n",
    "        range(len(indices)),\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Create training matrix\n",
    "    train_matrix = sparse_matrix.copy()\n",
    "    train_matrix.data = np.zeros_like(train_matrix.data)\n",
    "    for idx in train_indices:\n",
    "        row, col = indices[idx]\n",
    "        train_matrix[row, col] = values[idx]\n",
    "\n",
    "    # Create test matrix\n",
    "    test_matrix = sparse_matrix.copy()\n",
    "    test_matrix.data = np.zeros_like(test_matrix.data)\n",
    "    for idx in test_indices:\n",
    "        row, col = indices[idx]\n",
    "        test_matrix[row, col] = values[idx]\n",
    "\n",
    "    # Convert to CSR format for efficiency\n",
    "    train_matrix = train_matrix.tocsr()\n",
    "    test_matrix = test_matrix.tocsr()\n",
    "\n",
    "    return train_matrix, test_matrix\n",
    "\n",
    "train_matrix, test_matrix = split_train_test(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix density: 0.000043\n",
      "Training matrix density: 0.000043\n",
      "Test matrix density: 0.000043\n"
     ]
    }
   ],
   "source": [
    "# Print statistics about the split\n",
    "print(\n",
    "    f\"Original matrix density: {sparse_matrix.nnz / (sparse_matrix.shape[0] * sparse_matrix.shape[1]):.6f}\")\n",
    "print(\n",
    "    f\"Training matrix density: {train_matrix.nnz / (train_matrix.shape[0] * train_matrix.shape[1]):.6f}\")\n",
    "print(\n",
    "    f\"Test matrix density: {test_matrix.nnz / (test_matrix.shape[0] * test_matrix.shape[1]):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PratilipiDataset(Dataset):\n",
    "    def __init__(self, user_indices, item_indices, ratings):\n",
    "        self.user_indices = torch.LongTensor(user_indices)\n",
    "        self.item_indices = torch.LongTensor(item_indices)\n",
    "        # Normalize ratings to 0-1 range\n",
    "        self.ratings = torch.FloatTensor(\n",
    "            ratings) / 100.0  # Since read_percent is 0-100\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)  # Return the number of samples in the dataset\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.user_indices[idx], self.item_indices[idx], self.ratings[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=50):\n",
    "        super(NCF, self).__init__()\n",
    "\n",
    "        # Embedding layers\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "        # MLP layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(2 * embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, user_input, item_input):\n",
    "    # Get embeddings\n",
    "    user_embedded = self.user_embedding(user_input)\n",
    "    item_embedded = self.item_embedding(item_input)\n",
    "\n",
    "    # Concatenate embeddings\n",
    "    vector = torch.cat([user_embedded, item_embedded], dim=-1)\n",
    "\n",
    "    # Pass through MLP\n",
    "    prediction = self.fc_layers(vector)\n",
    "    return prediction.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for users, items, ratings in test_loader:\n",
    "            users = users.to(device)\n",
    "            items = items.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "\n",
    "            predictions = model(users, items)\n",
    "            loss = criterion(predictions, ratings)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split data into train and validation\n",
    "train_size = int(0.8 * len(user_indices))\n",
    "train_dataset = PratilipiDataset(\n",
    "    user_indices[:train_size],\n",
    "    item_indices[:train_size],\n",
    "    ratings[:train_size]\n",
    ")\n",
    "val_dataset = PratilipiDataset(\n",
    "    user_indices[train_size:],\n",
    "    item_indices[train_size:],\n",
    "    ratings[train_size:]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10, lr=0.001, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, 'min', patience=2)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (users, items, ratings) in enumerate(train_loader):\n",
    "            users = users.to(device)\n",
    "            items = items.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "\n",
    "            predictions = model(users, items)\n",
    "            loss = criterion(predictions, ratings)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(\n",
    "                    f'Epoch {epoch+1}/{epochs} - Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}')\n",
    "\n",
    "        # Validation\n",
    "        val_loss = evaluate_model(model, val_loader, device)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Training Loss: {train_loss/len(train_loader):.4f}')\n",
    "        print(f'Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10 - Batch 0/7813 - Loss: 0.2560\n",
      "Epoch 1/10 - Batch 100/7813 - Loss: 0.0630\n",
      "Epoch 1/10 - Batch 200/7813 - Loss: 0.0629\n",
      "Epoch 1/10 - Batch 300/7813 - Loss: 0.0531\n",
      "Epoch 1/10 - Batch 400/7813 - Loss: 0.0446\n",
      "Epoch 1/10 - Batch 500/7813 - Loss: 0.0325\n",
      "Epoch 1/10 - Batch 600/7813 - Loss: 0.0500\n",
      "Epoch 1/10 - Batch 700/7813 - Loss: 0.0571\n",
      "Epoch 1/10 - Batch 800/7813 - Loss: 0.0641\n",
      "Epoch 1/10 - Batch 900/7813 - Loss: 0.0585\n",
      "Epoch 1/10 - Batch 1000/7813 - Loss: 0.0385\n",
      "Epoch 1/10 - Batch 1100/7813 - Loss: 0.0429\n",
      "Epoch 1/10 - Batch 1200/7813 - Loss: 0.0395\n",
      "Epoch 1/10 - Batch 1300/7813 - Loss: 0.0581\n",
      "Epoch 1/10 - Batch 1400/7813 - Loss: 0.0302\n",
      "Epoch 1/10 - Batch 1500/7813 - Loss: 0.0482\n",
      "Epoch 1/10 - Batch 1600/7813 - Loss: 0.0507\n",
      "Epoch 1/10 - Batch 1700/7813 - Loss: 0.0611\n",
      "Epoch 1/10 - Batch 1800/7813 - Loss: 0.0393\n",
      "Epoch 1/10 - Batch 1900/7813 - Loss: 0.0365\n",
      "Epoch 1/10 - Batch 2000/7813 - Loss: 0.0483\n",
      "Epoch 1/10 - Batch 2100/7813 - Loss: 0.0557\n",
      "Epoch 1/10 - Batch 2200/7813 - Loss: 0.0281\n",
      "Epoch 1/10 - Batch 2300/7813 - Loss: 0.0473\n",
      "Epoch 1/10 - Batch 2400/7813 - Loss: 0.0423\n",
      "Epoch 1/10 - Batch 2500/7813 - Loss: 0.0463\n",
      "Epoch 1/10 - Batch 2600/7813 - Loss: 0.0502\n",
      "Epoch 1/10 - Batch 2700/7813 - Loss: 0.0417\n",
      "Epoch 1/10 - Batch 2800/7813 - Loss: 0.0309\n",
      "Epoch 1/10 - Batch 2900/7813 - Loss: 0.0392\n",
      "Epoch 1/10 - Batch 3000/7813 - Loss: 0.0350\n",
      "Epoch 1/10 - Batch 3100/7813 - Loss: 0.0480\n",
      "Epoch 1/10 - Batch 3200/7813 - Loss: 0.0500\n",
      "Epoch 1/10 - Batch 3300/7813 - Loss: 0.0440\n",
      "Epoch 1/10 - Batch 3400/7813 - Loss: 0.0506\n",
      "Epoch 1/10 - Batch 3500/7813 - Loss: 0.0605\n",
      "Epoch 1/10 - Batch 3600/7813 - Loss: 0.0454\n",
      "Epoch 1/10 - Batch 3700/7813 - Loss: 0.0422\n",
      "Epoch 1/10 - Batch 3800/7813 - Loss: 0.0482\n",
      "Epoch 1/10 - Batch 3900/7813 - Loss: 0.0302\n",
      "Epoch 1/10 - Batch 4000/7813 - Loss: 0.0471\n",
      "Epoch 1/10 - Batch 4100/7813 - Loss: 0.0373\n",
      "Epoch 1/10 - Batch 4200/7813 - Loss: 0.0410\n",
      "Epoch 1/10 - Batch 4300/7813 - Loss: 0.0470\n",
      "Epoch 1/10 - Batch 4400/7813 - Loss: 0.0301\n",
      "Epoch 1/10 - Batch 4500/7813 - Loss: 0.0476\n",
      "Epoch 1/10 - Batch 4600/7813 - Loss: 0.0581\n",
      "Epoch 1/10 - Batch 4700/7813 - Loss: 0.0544\n",
      "Epoch 1/10 - Batch 4800/7813 - Loss: 0.0547\n",
      "Epoch 1/10 - Batch 4900/7813 - Loss: 0.0363\n",
      "Epoch 1/10 - Batch 5000/7813 - Loss: 0.0271\n",
      "Epoch 1/10 - Batch 5100/7813 - Loss: 0.0303\n",
      "Epoch 1/10 - Batch 5200/7813 - Loss: 0.0350\n",
      "Epoch 1/10 - Batch 5300/7813 - Loss: 0.0531\n",
      "Epoch 1/10 - Batch 5400/7813 - Loss: 0.0593\n",
      "Epoch 1/10 - Batch 5500/7813 - Loss: 0.0393\n",
      "Epoch 1/10 - Batch 5600/7813 - Loss: 0.0570\n",
      "Epoch 1/10 - Batch 5700/7813 - Loss: 0.0482\n",
      "Epoch 1/10 - Batch 5800/7813 - Loss: 0.0396\n",
      "Epoch 1/10 - Batch 5900/7813 - Loss: 0.0513\n",
      "Epoch 1/10 - Batch 6000/7813 - Loss: 0.0282\n",
      "Epoch 1/10 - Batch 6100/7813 - Loss: 0.0518\n",
      "Epoch 1/10 - Batch 6200/7813 - Loss: 0.0475\n",
      "Epoch 1/10 - Batch 6300/7813 - Loss: 0.0383\n",
      "Epoch 1/10 - Batch 6400/7813 - Loss: 0.0502\n",
      "Epoch 1/10 - Batch 6500/7813 - Loss: 0.0343\n",
      "Epoch 1/10 - Batch 6600/7813 - Loss: 0.0345\n",
      "Epoch 1/10 - Batch 6700/7813 - Loss: 0.0397\n",
      "Epoch 1/10 - Batch 6800/7813 - Loss: 0.0342\n",
      "Epoch 1/10 - Batch 6900/7813 - Loss: 0.0471\n",
      "Epoch 1/10 - Batch 7000/7813 - Loss: 0.0385\n",
      "Epoch 1/10 - Batch 7100/7813 - Loss: 0.0359\n",
      "Epoch 1/10 - Batch 7200/7813 - Loss: 0.0234\n",
      "Epoch 1/10 - Batch 7300/7813 - Loss: 0.0372\n",
      "Epoch 1/10 - Batch 7400/7813 - Loss: 0.0281\n",
      "Epoch 1/10 - Batch 7500/7813 - Loss: 0.0443\n",
      "Epoch 1/10 - Batch 7600/7813 - Loss: 0.0363\n",
      "Epoch 1/10 - Batch 7700/7813 - Loss: 0.0361\n",
      "Epoch 1/10 - Batch 7800/7813 - Loss: 0.0397\n",
      "Epoch 1/10:\n",
      "Training Loss: 0.0444\n",
      "Validation Loss: 0.0472\n",
      "Epoch 2/10 - Batch 0/7813 - Loss: 0.0173\n",
      "Epoch 2/10 - Batch 100/7813 - Loss: 0.0386\n",
      "Epoch 2/10 - Batch 200/7813 - Loss: 0.0406\n",
      "Epoch 2/10 - Batch 300/7813 - Loss: 0.0243\n",
      "Epoch 2/10 - Batch 400/7813 - Loss: 0.0358\n",
      "Epoch 2/10 - Batch 500/7813 - Loss: 0.0364\n",
      "Epoch 2/10 - Batch 600/7813 - Loss: 0.0493\n",
      "Epoch 2/10 - Batch 700/7813 - Loss: 0.0480\n",
      "Epoch 2/10 - Batch 800/7813 - Loss: 0.0444\n",
      "Epoch 2/10 - Batch 900/7813 - Loss: 0.0295\n",
      "Epoch 2/10 - Batch 1000/7813 - Loss: 0.0396\n",
      "Epoch 2/10 - Batch 1100/7813 - Loss: 0.0286\n",
      "Epoch 2/10 - Batch 1200/7813 - Loss: 0.0296\n",
      "Epoch 2/10 - Batch 1300/7813 - Loss: 0.0590\n",
      "Epoch 2/10 - Batch 1400/7813 - Loss: 0.0439\n",
      "Epoch 2/10 - Batch 1500/7813 - Loss: 0.0325\n",
      "Epoch 2/10 - Batch 1600/7813 - Loss: 0.0291\n",
      "Epoch 2/10 - Batch 1700/7813 - Loss: 0.0327\n",
      "Epoch 2/10 - Batch 1800/7813 - Loss: 0.0367\n",
      "Epoch 2/10 - Batch 1900/7813 - Loss: 0.0316\n",
      "Epoch 2/10 - Batch 2000/7813 - Loss: 0.0420\n",
      "Epoch 2/10 - Batch 2100/7813 - Loss: 0.0356\n",
      "Epoch 2/10 - Batch 2200/7813 - Loss: 0.0335\n",
      "Epoch 2/10 - Batch 2300/7813 - Loss: 0.0570\n",
      "Epoch 2/10 - Batch 2400/7813 - Loss: 0.0388\n",
      "Epoch 2/10 - Batch 2500/7813 - Loss: 0.0302\n",
      "Epoch 2/10 - Batch 2600/7813 - Loss: 0.0449\n",
      "Epoch 2/10 - Batch 2700/7813 - Loss: 0.0353\n",
      "Epoch 2/10 - Batch 2800/7813 - Loss: 0.0463\n",
      "Epoch 2/10 - Batch 2900/7813 - Loss: 0.0420\n",
      "Epoch 2/10 - Batch 3000/7813 - Loss: 0.0621\n",
      "Epoch 2/10 - Batch 3100/7813 - Loss: 0.0253\n",
      "Epoch 2/10 - Batch 3200/7813 - Loss: 0.0294\n",
      "Epoch 2/10 - Batch 3300/7813 - Loss: 0.0296\n",
      "Epoch 2/10 - Batch 3400/7813 - Loss: 0.0404\n",
      "Epoch 2/10 - Batch 3500/7813 - Loss: 0.0310\n",
      "Epoch 2/10 - Batch 3600/7813 - Loss: 0.0532\n",
      "Epoch 2/10 - Batch 3700/7813 - Loss: 0.0204\n",
      "Epoch 2/10 - Batch 3800/7813 - Loss: 0.0361\n",
      "Epoch 2/10 - Batch 3900/7813 - Loss: 0.0377\n",
      "Epoch 2/10 - Batch 4000/7813 - Loss: 0.0325\n",
      "Epoch 2/10 - Batch 4100/7813 - Loss: 0.0356\n",
      "Epoch 2/10 - Batch 4200/7813 - Loss: 0.0333\n",
      "Epoch 2/10 - Batch 4300/7813 - Loss: 0.0312\n",
      "Epoch 2/10 - Batch 4400/7813 - Loss: 0.0459\n",
      "Epoch 2/10 - Batch 4500/7813 - Loss: 0.0401\n",
      "Epoch 2/10 - Batch 4600/7813 - Loss: 0.0463\n",
      "Epoch 2/10 - Batch 4700/7813 - Loss: 0.0356\n",
      "Epoch 2/10 - Batch 4800/7813 - Loss: 0.0457\n",
      "Epoch 2/10 - Batch 4900/7813 - Loss: 0.0394\n",
      "Epoch 2/10 - Batch 5000/7813 - Loss: 0.0483\n",
      "Epoch 2/10 - Batch 5100/7813 - Loss: 0.0415\n",
      "Epoch 2/10 - Batch 5200/7813 - Loss: 0.0330\n",
      "Epoch 2/10 - Batch 5300/7813 - Loss: 0.0369\n",
      "Epoch 2/10 - Batch 5400/7813 - Loss: 0.0362\n",
      "Epoch 2/10 - Batch 5500/7813 - Loss: 0.0411\n",
      "Epoch 2/10 - Batch 5600/7813 - Loss: 0.0361\n",
      "Epoch 2/10 - Batch 5700/7813 - Loss: 0.0368\n",
      "Epoch 2/10 - Batch 5800/7813 - Loss: 0.0316\n",
      "Epoch 2/10 - Batch 5900/7813 - Loss: 0.0481\n",
      "Epoch 2/10 - Batch 6000/7813 - Loss: 0.0353\n",
      "Epoch 2/10 - Batch 6100/7813 - Loss: 0.0482\n",
      "Epoch 2/10 - Batch 6200/7813 - Loss: 0.0422\n",
      "Epoch 2/10 - Batch 6300/7813 - Loss: 0.0387\n",
      "Epoch 2/10 - Batch 6400/7813 - Loss: 0.0343\n",
      "Epoch 2/10 - Batch 6500/7813 - Loss: 0.0435\n",
      "Epoch 2/10 - Batch 6600/7813 - Loss: 0.0428\n",
      "Epoch 2/10 - Batch 6700/7813 - Loss: 0.0540\n",
      "Epoch 2/10 - Batch 6800/7813 - Loss: 0.0491\n",
      "Epoch 2/10 - Batch 6900/7813 - Loss: 0.0602\n",
      "Epoch 2/10 - Batch 7000/7813 - Loss: 0.0392\n",
      "Epoch 2/10 - Batch 7100/7813 - Loss: 0.0421\n",
      "Epoch 2/10 - Batch 7200/7813 - Loss: 0.0352\n",
      "Epoch 2/10 - Batch 7300/7813 - Loss: 0.0297\n",
      "Epoch 2/10 - Batch 7400/7813 - Loss: 0.0405\n",
      "Epoch 2/10 - Batch 7500/7813 - Loss: 0.0298\n",
      "Epoch 2/10 - Batch 7600/7813 - Loss: 0.0391\n",
      "Epoch 2/10 - Batch 7700/7813 - Loss: 0.0525\n",
      "Epoch 2/10 - Batch 7800/7813 - Loss: 0.0426\n",
      "Epoch 2/10:\n",
      "Training Loss: 0.0405\n",
      "Validation Loss: 0.0467\n",
      "Epoch 3/10 - Batch 0/7813 - Loss: 0.0385\n",
      "Epoch 3/10 - Batch 100/7813 - Loss: 0.0283\n",
      "Epoch 3/10 - Batch 200/7813 - Loss: 0.0424\n",
      "Epoch 3/10 - Batch 300/7813 - Loss: 0.0394\n",
      "Epoch 3/10 - Batch 400/7813 - Loss: 0.0366\n",
      "Epoch 3/10 - Batch 500/7813 - Loss: 0.0433\n",
      "Epoch 3/10 - Batch 600/7813 - Loss: 0.0367\n",
      "Epoch 3/10 - Batch 700/7813 - Loss: 0.0505\n",
      "Epoch 3/10 - Batch 800/7813 - Loss: 0.0350\n",
      "Epoch 3/10 - Batch 900/7813 - Loss: 0.0266\n",
      "Epoch 3/10 - Batch 1000/7813 - Loss: 0.0342\n",
      "Epoch 3/10 - Batch 1100/7813 - Loss: 0.0330\n",
      "Epoch 3/10 - Batch 1200/7813 - Loss: 0.0399\n",
      "Epoch 3/10 - Batch 1300/7813 - Loss: 0.0563\n",
      "Epoch 3/10 - Batch 1400/7813 - Loss: 0.0410\n",
      "Epoch 3/10 - Batch 1500/7813 - Loss: 0.0375\n",
      "Epoch 3/10 - Batch 1600/7813 - Loss: 0.0354\n",
      "Epoch 3/10 - Batch 1700/7813 - Loss: 0.0478\n",
      "Epoch 3/10 - Batch 1800/7813 - Loss: 0.0361\n",
      "Epoch 3/10 - Batch 1900/7813 - Loss: 0.0459\n",
      "Epoch 3/10 - Batch 2000/7813 - Loss: 0.0398\n",
      "Epoch 3/10 - Batch 2100/7813 - Loss: 0.0258\n",
      "Epoch 3/10 - Batch 2200/7813 - Loss: 0.0492\n",
      "Epoch 3/10 - Batch 2300/7813 - Loss: 0.0303\n",
      "Epoch 3/10 - Batch 2400/7813 - Loss: 0.0347\n",
      "Epoch 3/10 - Batch 2500/7813 - Loss: 0.0364\n",
      "Epoch 3/10 - Batch 2600/7813 - Loss: 0.0346\n",
      "Epoch 3/10 - Batch 2700/7813 - Loss: 0.0319\n",
      "Epoch 3/10 - Batch 2800/7813 - Loss: 0.0560\n",
      "Epoch 3/10 - Batch 2900/7813 - Loss: 0.0461\n",
      "Epoch 3/10 - Batch 3000/7813 - Loss: 0.0501\n",
      "Epoch 3/10 - Batch 3100/7813 - Loss: 0.0357\n",
      "Epoch 3/10 - Batch 3200/7813 - Loss: 0.0513\n",
      "Epoch 3/10 - Batch 3300/7813 - Loss: 0.0232\n",
      "Epoch 3/10 - Batch 3400/7813 - Loss: 0.0516\n",
      "Epoch 3/10 - Batch 3500/7813 - Loss: 0.0502\n",
      "Epoch 3/10 - Batch 3600/7813 - Loss: 0.0393\n",
      "Epoch 3/10 - Batch 3700/7813 - Loss: 0.0496\n",
      "Epoch 3/10 - Batch 3800/7813 - Loss: 0.0364\n",
      "Epoch 3/10 - Batch 3900/7813 - Loss: 0.0348\n",
      "Epoch 3/10 - Batch 4000/7813 - Loss: 0.0468\n",
      "Epoch 3/10 - Batch 4100/7813 - Loss: 0.0463\n",
      "Epoch 3/10 - Batch 4200/7813 - Loss: 0.0310\n",
      "Epoch 3/10 - Batch 4300/7813 - Loss: 0.0226\n",
      "Epoch 3/10 - Batch 4400/7813 - Loss: 0.0439\n",
      "Epoch 3/10 - Batch 4500/7813 - Loss: 0.0323\n",
      "Epoch 3/10 - Batch 4600/7813 - Loss: 0.0399\n",
      "Epoch 3/10 - Batch 4700/7813 - Loss: 0.0345\n",
      "Epoch 3/10 - Batch 4800/7813 - Loss: 0.0481\n",
      "Epoch 3/10 - Batch 4900/7813 - Loss: 0.0533\n",
      "Epoch 3/10 - Batch 5000/7813 - Loss: 0.0477\n",
      "Epoch 3/10 - Batch 5100/7813 - Loss: 0.0367\n",
      "Epoch 3/10 - Batch 5200/7813 - Loss: 0.0360\n",
      "Epoch 3/10 - Batch 5300/7813 - Loss: 0.0421\n",
      "Epoch 3/10 - Batch 5400/7813 - Loss: 0.0417\n",
      "Epoch 3/10 - Batch 5500/7813 - Loss: 0.0447\n",
      "Epoch 3/10 - Batch 5600/7813 - Loss: 0.0246\n",
      "Epoch 3/10 - Batch 5700/7813 - Loss: 0.0354\n",
      "Epoch 3/10 - Batch 5800/7813 - Loss: 0.0291\n",
      "Epoch 3/10 - Batch 5900/7813 - Loss: 0.0355\n",
      "Epoch 3/10 - Batch 6000/7813 - Loss: 0.0420\n",
      "Epoch 3/10 - Batch 6100/7813 - Loss: 0.0477\n",
      "Epoch 3/10 - Batch 6200/7813 - Loss: 0.0346\n",
      "Epoch 3/10 - Batch 6300/7813 - Loss: 0.0253\n",
      "Epoch 3/10 - Batch 6400/7813 - Loss: 0.0389\n",
      "Epoch 3/10 - Batch 6500/7813 - Loss: 0.0412\n",
      "Epoch 3/10 - Batch 6600/7813 - Loss: 0.0541\n",
      "Epoch 3/10 - Batch 6700/7813 - Loss: 0.0488\n",
      "Epoch 3/10 - Batch 6800/7813 - Loss: 0.0191\n",
      "Epoch 3/10 - Batch 6900/7813 - Loss: 0.0447\n",
      "Epoch 3/10 - Batch 7000/7813 - Loss: 0.0488\n",
      "Epoch 3/10 - Batch 7100/7813 - Loss: 0.0348\n",
      "Epoch 3/10 - Batch 7200/7813 - Loss: 0.0419\n",
      "Epoch 3/10 - Batch 7300/7813 - Loss: 0.0506\n",
      "Epoch 3/10 - Batch 7400/7813 - Loss: 0.0367\n",
      "Epoch 3/10 - Batch 7500/7813 - Loss: 0.0424\n",
      "Epoch 3/10 - Batch 7600/7813 - Loss: 0.0420\n",
      "Epoch 3/10 - Batch 7700/7813 - Loss: 0.0390\n",
      "Epoch 3/10 - Batch 7800/7813 - Loss: 0.0371\n",
      "Epoch 3/10:\n",
      "Training Loss: 0.0391\n",
      "Validation Loss: 0.0467\n",
      "Epoch 4/10 - Batch 0/7813 - Loss: 0.0250\n",
      "Epoch 4/10 - Batch 100/7813 - Loss: 0.0378\n",
      "Epoch 4/10 - Batch 200/7813 - Loss: 0.0262\n",
      "Epoch 4/10 - Batch 300/7813 - Loss: 0.0206\n",
      "Epoch 4/10 - Batch 400/7813 - Loss: 0.0251\n",
      "Epoch 4/10 - Batch 500/7813 - Loss: 0.0355\n",
      "Epoch 4/10 - Batch 600/7813 - Loss: 0.0305\n",
      "Epoch 4/10 - Batch 700/7813 - Loss: 0.0272\n",
      "Epoch 4/10 - Batch 800/7813 - Loss: 0.0339\n",
      "Epoch 4/10 - Batch 900/7813 - Loss: 0.0278\n",
      "Epoch 4/10 - Batch 1000/7813 - Loss: 0.0307\n",
      "Epoch 4/10 - Batch 1100/7813 - Loss: 0.0380\n",
      "Epoch 4/10 - Batch 1200/7813 - Loss: 0.0375\n",
      "Epoch 4/10 - Batch 1300/7813 - Loss: 0.0276\n",
      "Epoch 4/10 - Batch 1400/7813 - Loss: 0.0423\n",
      "Epoch 4/10 - Batch 1500/7813 - Loss: 0.0398\n",
      "Epoch 4/10 - Batch 1600/7813 - Loss: 0.0264\n",
      "Epoch 4/10 - Batch 1700/7813 - Loss: 0.0358\n",
      "Epoch 4/10 - Batch 1800/7813 - Loss: 0.0343\n",
      "Epoch 4/10 - Batch 1900/7813 - Loss: 0.0268\n",
      "Epoch 4/10 - Batch 2000/7813 - Loss: 0.0328\n",
      "Epoch 4/10 - Batch 2100/7813 - Loss: 0.0425\n",
      "Epoch 4/10 - Batch 2200/7813 - Loss: 0.0278\n",
      "Epoch 4/10 - Batch 2300/7813 - Loss: 0.0300\n",
      "Epoch 4/10 - Batch 2400/7813 - Loss: 0.0338\n",
      "Epoch 4/10 - Batch 2500/7813 - Loss: 0.0379\n",
      "Epoch 4/10 - Batch 2600/7813 - Loss: 0.0243\n",
      "Epoch 4/10 - Batch 2700/7813 - Loss: 0.0469\n",
      "Epoch 4/10 - Batch 2800/7813 - Loss: 0.0332\n",
      "Epoch 4/10 - Batch 2900/7813 - Loss: 0.0412\n",
      "Epoch 4/10 - Batch 3000/7813 - Loss: 0.0304\n",
      "Epoch 4/10 - Batch 3100/7813 - Loss: 0.0214\n",
      "Epoch 4/10 - Batch 3200/7813 - Loss: 0.0235\n",
      "Epoch 4/10 - Batch 3300/7813 - Loss: 0.0476\n",
      "Epoch 4/10 - Batch 3400/7813 - Loss: 0.0424\n",
      "Epoch 4/10 - Batch 3500/7813 - Loss: 0.0334\n",
      "Epoch 4/10 - Batch 3600/7813 - Loss: 0.0462\n",
      "Epoch 4/10 - Batch 3700/7813 - Loss: 0.0348\n",
      "Epoch 4/10 - Batch 3800/7813 - Loss: 0.0387\n",
      "Epoch 4/10 - Batch 3900/7813 - Loss: 0.0234\n",
      "Epoch 4/10 - Batch 4000/7813 - Loss: 0.0377\n",
      "Epoch 4/10 - Batch 4100/7813 - Loss: 0.0404\n",
      "Epoch 4/10 - Batch 4200/7813 - Loss: 0.0353\n",
      "Epoch 4/10 - Batch 4300/7813 - Loss: 0.0321\n",
      "Epoch 4/10 - Batch 4400/7813 - Loss: 0.0383\n",
      "Epoch 4/10 - Batch 4500/7813 - Loss: 0.0376\n",
      "Epoch 4/10 - Batch 4600/7813 - Loss: 0.0366\n",
      "Epoch 4/10 - Batch 4700/7813 - Loss: 0.0413\n",
      "Epoch 4/10 - Batch 4800/7813 - Loss: 0.0444\n",
      "Epoch 4/10 - Batch 4900/7813 - Loss: 0.0398\n",
      "Epoch 4/10 - Batch 5000/7813 - Loss: 0.0392\n",
      "Epoch 4/10 - Batch 5100/7813 - Loss: 0.0514\n",
      "Epoch 4/10 - Batch 5200/7813 - Loss: 0.0414\n",
      "Epoch 4/10 - Batch 5300/7813 - Loss: 0.0437\n",
      "Epoch 4/10 - Batch 5400/7813 - Loss: 0.0451\n",
      "Epoch 4/10 - Batch 5500/7813 - Loss: 0.0500\n",
      "Epoch 4/10 - Batch 5600/7813 - Loss: 0.0502\n",
      "Epoch 4/10 - Batch 5700/7813 - Loss: 0.0377\n",
      "Epoch 4/10 - Batch 5800/7813 - Loss: 0.0475\n",
      "Epoch 4/10 - Batch 5900/7813 - Loss: 0.0352\n",
      "Epoch 4/10 - Batch 6000/7813 - Loss: 0.0521\n",
      "Epoch 4/10 - Batch 6100/7813 - Loss: 0.0328\n",
      "Epoch 4/10 - Batch 6200/7813 - Loss: 0.0399\n",
      "Epoch 4/10 - Batch 6300/7813 - Loss: 0.0579\n",
      "Epoch 4/10 - Batch 6400/7813 - Loss: 0.0273\n",
      "Epoch 4/10 - Batch 6500/7813 - Loss: 0.0373\n",
      "Epoch 4/10 - Batch 6600/7813 - Loss: 0.0359\n",
      "Epoch 4/10 - Batch 6700/7813 - Loss: 0.0362\n",
      "Epoch 4/10 - Batch 6800/7813 - Loss: 0.0398\n",
      "Epoch 4/10 - Batch 6900/7813 - Loss: 0.0398\n",
      "Epoch 4/10 - Batch 7000/7813 - Loss: 0.0420\n",
      "Epoch 4/10 - Batch 7100/7813 - Loss: 0.0257\n",
      "Epoch 4/10 - Batch 7200/7813 - Loss: 0.0466\n",
      "Epoch 4/10 - Batch 7300/7813 - Loss: 0.0321\n",
      "Epoch 4/10 - Batch 7400/7813 - Loss: 0.0271\n",
      "Epoch 4/10 - Batch 7500/7813 - Loss: 0.0404\n",
      "Epoch 4/10 - Batch 7600/7813 - Loss: 0.0465\n",
      "Epoch 4/10 - Batch 7700/7813 - Loss: 0.0388\n",
      "Epoch 4/10 - Batch 7800/7813 - Loss: 0.0483\n",
      "Epoch 4/10:\n",
      "Training Loss: 0.0381\n",
      "Validation Loss: 0.0465\n",
      "Epoch 5/10 - Batch 0/7813 - Loss: 0.0439\n",
      "Epoch 5/10 - Batch 100/7813 - Loss: 0.0179\n",
      "Epoch 5/10 - Batch 200/7813 - Loss: 0.0328\n",
      "Epoch 5/10 - Batch 300/7813 - Loss: 0.0412\n",
      "Epoch 5/10 - Batch 400/7813 - Loss: 0.0276\n",
      "Epoch 5/10 - Batch 500/7813 - Loss: 0.0276\n",
      "Epoch 5/10 - Batch 600/7813 - Loss: 0.0344\n",
      "Epoch 5/10 - Batch 700/7813 - Loss: 0.0338\n",
      "Epoch 5/10 - Batch 800/7813 - Loss: 0.0331\n",
      "Epoch 5/10 - Batch 900/7813 - Loss: 0.0305\n",
      "Epoch 5/10 - Batch 1000/7813 - Loss: 0.0382\n",
      "Epoch 5/10 - Batch 1100/7813 - Loss: 0.0195\n",
      "Epoch 5/10 - Batch 1200/7813 - Loss: 0.0282\n",
      "Epoch 5/10 - Batch 1300/7813 - Loss: 0.0511\n",
      "Epoch 5/10 - Batch 1400/7813 - Loss: 0.0319\n",
      "Epoch 5/10 - Batch 1500/7813 - Loss: 0.0353\n",
      "Epoch 5/10 - Batch 1600/7813 - Loss: 0.0429\n",
      "Epoch 5/10 - Batch 1700/7813 - Loss: 0.0219\n",
      "Epoch 5/10 - Batch 1800/7813 - Loss: 0.0281\n",
      "Epoch 5/10 - Batch 1900/7813 - Loss: 0.0365\n",
      "Epoch 5/10 - Batch 2000/7813 - Loss: 0.0521\n",
      "Epoch 5/10 - Batch 2100/7813 - Loss: 0.0321\n",
      "Epoch 5/10 - Batch 2200/7813 - Loss: 0.0420\n",
      "Epoch 5/10 - Batch 2300/7813 - Loss: 0.0311\n",
      "Epoch 5/10 - Batch 2400/7813 - Loss: 0.0416\n",
      "Epoch 5/10 - Batch 2500/7813 - Loss: 0.0253\n",
      "Epoch 5/10 - Batch 2600/7813 - Loss: 0.0226\n",
      "Epoch 5/10 - Batch 2700/7813 - Loss: 0.0283\n",
      "Epoch 5/10 - Batch 2800/7813 - Loss: 0.0512\n",
      "Epoch 5/10 - Batch 2900/7813 - Loss: 0.0433\n",
      "Epoch 5/10 - Batch 3000/7813 - Loss: 0.0460\n",
      "Epoch 5/10 - Batch 3100/7813 - Loss: 0.0492\n",
      "Epoch 5/10 - Batch 3200/7813 - Loss: 0.0394\n",
      "Epoch 5/10 - Batch 3300/7813 - Loss: 0.0390\n",
      "Epoch 5/10 - Batch 3400/7813 - Loss: 0.0436\n",
      "Epoch 5/10 - Batch 3500/7813 - Loss: 0.0323\n",
      "Epoch 5/10 - Batch 3600/7813 - Loss: 0.0332\n",
      "Epoch 5/10 - Batch 3700/7813 - Loss: 0.0482\n",
      "Epoch 5/10 - Batch 3800/7813 - Loss: 0.0283\n",
      "Epoch 5/10 - Batch 3900/7813 - Loss: 0.0392\n",
      "Epoch 5/10 - Batch 4000/7813 - Loss: 0.0377\n",
      "Epoch 5/10 - Batch 4100/7813 - Loss: 0.0336\n",
      "Epoch 5/10 - Batch 4200/7813 - Loss: 0.0422\n",
      "Epoch 5/10 - Batch 4300/7813 - Loss: 0.0320\n",
      "Epoch 5/10 - Batch 4400/7813 - Loss: 0.0422\n",
      "Epoch 5/10 - Batch 4500/7813 - Loss: 0.0316\n",
      "Epoch 5/10 - Batch 4600/7813 - Loss: 0.0438\n",
      "Epoch 5/10 - Batch 4700/7813 - Loss: 0.0340\n",
      "Epoch 5/10 - Batch 4800/7813 - Loss: 0.0543\n",
      "Epoch 5/10 - Batch 4900/7813 - Loss: 0.0377\n",
      "Epoch 5/10 - Batch 5000/7813 - Loss: 0.0333\n",
      "Epoch 5/10 - Batch 5100/7813 - Loss: 0.0351\n",
      "Epoch 5/10 - Batch 5200/7813 - Loss: 0.0341\n",
      "Epoch 5/10 - Batch 5300/7813 - Loss: 0.0501\n",
      "Epoch 5/10 - Batch 5400/7813 - Loss: 0.0569\n",
      "Epoch 5/10 - Batch 5500/7813 - Loss: 0.0379\n",
      "Epoch 5/10 - Batch 5600/7813 - Loss: 0.0290\n",
      "Epoch 5/10 - Batch 5700/7813 - Loss: 0.0447\n",
      "Epoch 5/10 - Batch 5800/7813 - Loss: 0.0309\n",
      "Epoch 5/10 - Batch 5900/7813 - Loss: 0.0392\n",
      "Epoch 5/10 - Batch 6000/7813 - Loss: 0.0382\n",
      "Epoch 5/10 - Batch 6100/7813 - Loss: 0.0354\n",
      "Epoch 5/10 - Batch 6200/7813 - Loss: 0.0401\n",
      "Epoch 5/10 - Batch 6300/7813 - Loss: 0.0368\n",
      "Epoch 5/10 - Batch 6400/7813 - Loss: 0.0477\n",
      "Epoch 5/10 - Batch 6500/7813 - Loss: 0.0397\n",
      "Epoch 5/10 - Batch 6600/7813 - Loss: 0.0414\n",
      "Epoch 5/10 - Batch 6700/7813 - Loss: 0.0480\n",
      "Epoch 5/10 - Batch 6800/7813 - Loss: 0.0313\n",
      "Epoch 5/10 - Batch 6900/7813 - Loss: 0.0401\n",
      "Epoch 5/10 - Batch 7000/7813 - Loss: 0.0378\n",
      "Epoch 5/10 - Batch 7100/7813 - Loss: 0.0382\n",
      "Epoch 5/10 - Batch 7200/7813 - Loss: 0.0335\n",
      "Epoch 5/10 - Batch 7300/7813 - Loss: 0.0325\n",
      "Epoch 5/10 - Batch 7400/7813 - Loss: 0.0398\n",
      "Epoch 5/10 - Batch 7500/7813 - Loss: 0.0430\n",
      "Epoch 5/10 - Batch 7600/7813 - Loss: 0.0520\n",
      "Epoch 5/10 - Batch 7700/7813 - Loss: 0.0502\n",
      "Epoch 5/10 - Batch 7800/7813 - Loss: 0.0353\n",
      "Epoch 5/10:\n",
      "Training Loss: 0.0378\n",
      "Validation Loss: 0.0469\n",
      "Epoch 6/10 - Batch 0/7813 - Loss: 0.0281\n",
      "Epoch 6/10 - Batch 100/7813 - Loss: 0.0187\n",
      "Epoch 6/10 - Batch 200/7813 - Loss: 0.0290\n",
      "Epoch 6/10 - Batch 300/7813 - Loss: 0.0363\n",
      "Epoch 6/10 - Batch 400/7813 - Loss: 0.0315\n",
      "Epoch 6/10 - Batch 500/7813 - Loss: 0.0271\n",
      "Epoch 6/10 - Batch 600/7813 - Loss: 0.0291\n",
      "Epoch 6/10 - Batch 700/7813 - Loss: 0.0296\n",
      "Epoch 6/10 - Batch 800/7813 - Loss: 0.0361\n",
      "Epoch 6/10 - Batch 900/7813 - Loss: 0.0273\n",
      "Epoch 6/10 - Batch 1000/7813 - Loss: 0.0373\n",
      "Epoch 6/10 - Batch 1100/7813 - Loss: 0.0245\n",
      "Epoch 6/10 - Batch 1200/7813 - Loss: 0.0276\n",
      "Epoch 6/10 - Batch 1300/7813 - Loss: 0.0401\n",
      "Epoch 6/10 - Batch 1400/7813 - Loss: 0.0276\n",
      "Epoch 6/10 - Batch 1500/7813 - Loss: 0.0320\n",
      "Epoch 6/10 - Batch 1600/7813 - Loss: 0.0352\n",
      "Epoch 6/10 - Batch 1700/7813 - Loss: 0.0358\n",
      "Epoch 6/10 - Batch 1800/7813 - Loss: 0.0403\n",
      "Epoch 6/10 - Batch 1900/7813 - Loss: 0.0439\n",
      "Epoch 6/10 - Batch 2000/7813 - Loss: 0.0266\n",
      "Epoch 6/10 - Batch 2100/7813 - Loss: 0.0346\n",
      "Epoch 6/10 - Batch 2200/7813 - Loss: 0.0454\n",
      "Epoch 6/10 - Batch 2300/7813 - Loss: 0.0343\n",
      "Epoch 6/10 - Batch 2400/7813 - Loss: 0.0311\n",
      "Epoch 6/10 - Batch 2500/7813 - Loss: 0.0353\n",
      "Epoch 6/10 - Batch 2600/7813 - Loss: 0.0292\n",
      "Epoch 6/10 - Batch 2700/7813 - Loss: 0.0207\n",
      "Epoch 6/10 - Batch 2800/7813 - Loss: 0.0456\n",
      "Epoch 6/10 - Batch 2900/7813 - Loss: 0.0271\n",
      "Epoch 6/10 - Batch 3000/7813 - Loss: 0.0343\n",
      "Epoch 6/10 - Batch 3100/7813 - Loss: 0.0384\n",
      "Epoch 6/10 - Batch 3200/7813 - Loss: 0.0319\n",
      "Epoch 6/10 - Batch 3300/7813 - Loss: 0.0433\n",
      "Epoch 6/10 - Batch 3400/7813 - Loss: 0.0254\n",
      "Epoch 6/10 - Batch 3500/7813 - Loss: 0.0255\n",
      "Epoch 6/10 - Batch 3600/7813 - Loss: 0.0279\n",
      "Epoch 6/10 - Batch 3700/7813 - Loss: 0.0388\n",
      "Epoch 6/10 - Batch 3800/7813 - Loss: 0.0364\n",
      "Epoch 6/10 - Batch 3900/7813 - Loss: 0.0283\n",
      "Epoch 6/10 - Batch 4000/7813 - Loss: 0.0394\n",
      "Epoch 6/10 - Batch 4100/7813 - Loss: 0.0424\n",
      "Epoch 6/10 - Batch 4200/7813 - Loss: 0.0376\n",
      "Epoch 6/10 - Batch 4300/7813 - Loss: 0.0606\n",
      "Epoch 6/10 - Batch 4400/7813 - Loss: 0.0447\n",
      "Epoch 6/10 - Batch 4500/7813 - Loss: 0.0450\n",
      "Epoch 6/10 - Batch 4600/7813 - Loss: 0.0402\n",
      "Epoch 6/10 - Batch 4700/7813 - Loss: 0.0486\n",
      "Epoch 6/10 - Batch 4800/7813 - Loss: 0.0444\n",
      "Epoch 6/10 - Batch 4900/7813 - Loss: 0.0402\n",
      "Epoch 6/10 - Batch 5000/7813 - Loss: 0.0385\n",
      "Epoch 6/10 - Batch 5100/7813 - Loss: 0.0511\n",
      "Epoch 6/10 - Batch 5200/7813 - Loss: 0.0390\n",
      "Epoch 6/10 - Batch 5300/7813 - Loss: 0.0316\n",
      "Epoch 6/10 - Batch 5400/7813 - Loss: 0.0387\n",
      "Epoch 6/10 - Batch 5500/7813 - Loss: 0.0390\n",
      "Epoch 6/10 - Batch 5600/7813 - Loss: 0.0339\n",
      "Epoch 6/10 - Batch 5700/7813 - Loss: 0.0511\n",
      "Epoch 6/10 - Batch 5800/7813 - Loss: 0.0497\n",
      "Epoch 6/10 - Batch 5900/7813 - Loss: 0.0421\n",
      "Epoch 6/10 - Batch 6000/7813 - Loss: 0.0392\n",
      "Epoch 6/10 - Batch 6100/7813 - Loss: 0.0413\n",
      "Epoch 6/10 - Batch 6200/7813 - Loss: 0.0529\n",
      "Epoch 6/10 - Batch 6300/7813 - Loss: 0.0393\n",
      "Epoch 6/10 - Batch 6400/7813 - Loss: 0.0431\n",
      "Epoch 6/10 - Batch 6500/7813 - Loss: 0.0383\n",
      "Epoch 6/10 - Batch 6600/7813 - Loss: 0.0441\n",
      "Epoch 6/10 - Batch 6700/7813 - Loss: 0.0287\n",
      "Epoch 6/10 - Batch 6800/7813 - Loss: 0.0453\n",
      "Epoch 6/10 - Batch 6900/7813 - Loss: 0.0408\n",
      "Epoch 6/10 - Batch 7000/7813 - Loss: 0.0434\n",
      "Epoch 6/10 - Batch 7100/7813 - Loss: 0.0433\n",
      "Epoch 6/10 - Batch 7200/7813 - Loss: 0.0321\n",
      "Epoch 6/10 - Batch 7300/7813 - Loss: 0.0373\n",
      "Epoch 6/10 - Batch 7400/7813 - Loss: 0.0335\n",
      "Epoch 6/10 - Batch 7500/7813 - Loss: 0.0324\n",
      "Epoch 6/10 - Batch 7600/7813 - Loss: 0.0289\n",
      "Epoch 6/10 - Batch 7700/7813 - Loss: 0.0492\n",
      "Epoch 6/10 - Batch 7800/7813 - Loss: 0.0288\n",
      "Epoch 6/10:\n",
      "Training Loss: 0.0377\n",
      "Validation Loss: 0.0465\n",
      "Epoch 7/10 - Batch 0/7813 - Loss: 0.0398\n",
      "Epoch 7/10 - Batch 100/7813 - Loss: 0.0321\n",
      "Epoch 7/10 - Batch 200/7813 - Loss: 0.0437\n",
      "Epoch 7/10 - Batch 300/7813 - Loss: 0.0265\n",
      "Epoch 7/10 - Batch 400/7813 - Loss: 0.0343\n",
      "Epoch 7/10 - Batch 500/7813 - Loss: 0.0324\n",
      "Epoch 7/10 - Batch 600/7813 - Loss: 0.0242\n",
      "Epoch 7/10 - Batch 700/7813 - Loss: 0.0428\n",
      "Epoch 7/10 - Batch 800/7813 - Loss: 0.0256\n",
      "Epoch 7/10 - Batch 900/7813 - Loss: 0.0342\n",
      "Epoch 7/10 - Batch 1000/7813 - Loss: 0.0319\n",
      "Epoch 7/10 - Batch 1100/7813 - Loss: 0.0375\n",
      "Epoch 7/10 - Batch 1200/7813 - Loss: 0.0379\n",
      "Epoch 7/10 - Batch 1300/7813 - Loss: 0.0536\n",
      "Epoch 7/10 - Batch 1400/7813 - Loss: 0.0346\n",
      "Epoch 7/10 - Batch 1500/7813 - Loss: 0.0217\n",
      "Epoch 7/10 - Batch 1600/7813 - Loss: 0.0398\n",
      "Epoch 7/10 - Batch 1700/7813 - Loss: 0.0341\n",
      "Epoch 7/10 - Batch 1800/7813 - Loss: 0.0184\n",
      "Epoch 7/10 - Batch 1900/7813 - Loss: 0.0243\n",
      "Epoch 7/10 - Batch 2000/7813 - Loss: 0.0317\n",
      "Epoch 7/10 - Batch 2100/7813 - Loss: 0.0504\n",
      "Epoch 7/10 - Batch 2200/7813 - Loss: 0.0250\n",
      "Epoch 7/10 - Batch 2300/7813 - Loss: 0.0480\n",
      "Epoch 7/10 - Batch 2400/7813 - Loss: 0.0463\n",
      "Epoch 7/10 - Batch 2500/7813 - Loss: 0.0323\n",
      "Epoch 7/10 - Batch 2600/7813 - Loss: 0.0295\n",
      "Epoch 7/10 - Batch 2700/7813 - Loss: 0.0303\n",
      "Epoch 7/10 - Batch 2800/7813 - Loss: 0.0349\n",
      "Epoch 7/10 - Batch 2900/7813 - Loss: 0.0196\n",
      "Epoch 7/10 - Batch 3000/7813 - Loss: 0.0488\n",
      "Epoch 7/10 - Batch 3100/7813 - Loss: 0.0433\n",
      "Epoch 7/10 - Batch 3200/7813 - Loss: 0.0461\n",
      "Epoch 7/10 - Batch 3300/7813 - Loss: 0.0483\n",
      "Epoch 7/10 - Batch 3400/7813 - Loss: 0.0336\n",
      "Epoch 7/10 - Batch 3500/7813 - Loss: 0.0327\n",
      "Epoch 7/10 - Batch 3600/7813 - Loss: 0.0484\n",
      "Epoch 7/10 - Batch 3700/7813 - Loss: 0.0322\n",
      "Epoch 7/10 - Batch 3800/7813 - Loss: 0.0497\n",
      "Epoch 7/10 - Batch 3900/7813 - Loss: 0.0439\n",
      "Epoch 7/10 - Batch 4000/7813 - Loss: 0.0338\n",
      "Epoch 7/10 - Batch 4100/7813 - Loss: 0.0277\n",
      "Epoch 7/10 - Batch 4200/7813 - Loss: 0.0451\n",
      "Epoch 7/10 - Batch 4300/7813 - Loss: 0.0401\n",
      "Epoch 7/10 - Batch 4400/7813 - Loss: 0.0423\n",
      "Epoch 7/10 - Batch 4500/7813 - Loss: 0.0433\n",
      "Epoch 7/10 - Batch 4600/7813 - Loss: 0.0542\n",
      "Epoch 7/10 - Batch 4700/7813 - Loss: 0.0321\n",
      "Epoch 7/10 - Batch 4800/7813 - Loss: 0.0436\n",
      "Epoch 7/10 - Batch 4900/7813 - Loss: 0.0336\n",
      "Epoch 7/10 - Batch 5000/7813 - Loss: 0.0378\n",
      "Epoch 7/10 - Batch 5100/7813 - Loss: 0.0427\n",
      "Epoch 7/10 - Batch 5200/7813 - Loss: 0.0416\n",
      "Epoch 7/10 - Batch 5300/7813 - Loss: 0.0397\n",
      "Epoch 7/10 - Batch 5400/7813 - Loss: 0.0299\n",
      "Epoch 7/10 - Batch 5500/7813 - Loss: 0.0376\n",
      "Epoch 7/10 - Batch 5600/7813 - Loss: 0.0411\n",
      "Epoch 7/10 - Batch 5700/7813 - Loss: 0.0328\n",
      "Epoch 7/10 - Batch 5800/7813 - Loss: 0.0293\n",
      "Epoch 7/10 - Batch 5900/7813 - Loss: 0.0468\n",
      "Epoch 7/10 - Batch 6000/7813 - Loss: 0.0348\n",
      "Epoch 7/10 - Batch 6100/7813 - Loss: 0.0361\n",
      "Epoch 7/10 - Batch 6200/7813 - Loss: 0.0364\n",
      "Epoch 7/10 - Batch 6300/7813 - Loss: 0.0343\n",
      "Epoch 7/10 - Batch 6400/7813 - Loss: 0.0445\n",
      "Epoch 7/10 - Batch 6500/7813 - Loss: 0.0379\n",
      "Epoch 7/10 - Batch 6600/7813 - Loss: 0.0337\n",
      "Epoch 7/10 - Batch 6700/7813 - Loss: 0.0202\n",
      "Epoch 7/10 - Batch 6800/7813 - Loss: 0.0290\n",
      "Epoch 7/10 - Batch 6900/7813 - Loss: 0.0357\n",
      "Epoch 7/10 - Batch 7000/7813 - Loss: 0.0418\n",
      "Epoch 7/10 - Batch 7100/7813 - Loss: 0.0461\n",
      "Epoch 7/10 - Batch 7200/7813 - Loss: 0.0464\n",
      "Epoch 7/10 - Batch 7300/7813 - Loss: 0.0418\n",
      "Epoch 7/10 - Batch 7400/7813 - Loss: 0.0425\n",
      "Epoch 7/10 - Batch 7500/7813 - Loss: 0.0297\n",
      "Epoch 7/10 - Batch 7600/7813 - Loss: 0.0487\n",
      "Epoch 7/10 - Batch 7700/7813 - Loss: 0.0384\n",
      "Epoch 7/10 - Batch 7800/7813 - Loss: 0.0414\n",
      "Epoch 7/10:\n",
      "Training Loss: 0.0377\n",
      "Validation Loss: 0.0467\n",
      "Epoch 8/10 - Batch 0/7813 - Loss: 0.0159\n",
      "Epoch 8/10 - Batch 100/7813 - Loss: 0.0282\n",
      "Epoch 8/10 - Batch 200/7813 - Loss: 0.0306\n",
      "Epoch 8/10 - Batch 300/7813 - Loss: 0.0384\n",
      "Epoch 8/10 - Batch 400/7813 - Loss: 0.0429\n",
      "Epoch 8/10 - Batch 500/7813 - Loss: 0.0365\n",
      "Epoch 8/10 - Batch 600/7813 - Loss: 0.0188\n",
      "Epoch 8/10 - Batch 700/7813 - Loss: 0.0341\n",
      "Epoch 8/10 - Batch 800/7813 - Loss: 0.0378\n",
      "Epoch 8/10 - Batch 900/7813 - Loss: 0.0320\n",
      "Epoch 8/10 - Batch 1000/7813 - Loss: 0.0334\n",
      "Epoch 8/10 - Batch 1100/7813 - Loss: 0.0273\n",
      "Epoch 8/10 - Batch 1200/7813 - Loss: 0.0381\n",
      "Epoch 8/10 - Batch 1300/7813 - Loss: 0.0468\n",
      "Epoch 8/10 - Batch 1400/7813 - Loss: 0.0409\n",
      "Epoch 8/10 - Batch 1500/7813 - Loss: 0.0356\n",
      "Epoch 8/10 - Batch 1600/7813 - Loss: 0.0431\n",
      "Epoch 8/10 - Batch 1700/7813 - Loss: 0.0342\n",
      "Epoch 8/10 - Batch 1800/7813 - Loss: 0.0287\n",
      "Epoch 8/10 - Batch 1900/7813 - Loss: 0.0345\n",
      "Epoch 8/10 - Batch 2000/7813 - Loss: 0.0523\n",
      "Epoch 8/10 - Batch 2100/7813 - Loss: 0.0358\n",
      "Epoch 8/10 - Batch 2200/7813 - Loss: 0.0297\n",
      "Epoch 8/10 - Batch 2300/7813 - Loss: 0.0260\n",
      "Epoch 8/10 - Batch 2400/7813 - Loss: 0.0437\n",
      "Epoch 8/10 - Batch 2500/7813 - Loss: 0.0419\n",
      "Epoch 8/10 - Batch 2600/7813 - Loss: 0.0321\n",
      "Epoch 8/10 - Batch 2700/7813 - Loss: 0.0354\n",
      "Epoch 8/10 - Batch 2800/7813 - Loss: 0.0457\n",
      "Epoch 8/10 - Batch 2900/7813 - Loss: 0.0319\n",
      "Epoch 8/10 - Batch 3000/7813 - Loss: 0.0330\n",
      "Epoch 8/10 - Batch 3100/7813 - Loss: 0.0356\n",
      "Epoch 8/10 - Batch 3200/7813 - Loss: 0.0344\n",
      "Epoch 8/10 - Batch 3300/7813 - Loss: 0.0358\n",
      "Epoch 8/10 - Batch 3400/7813 - Loss: 0.0310\n",
      "Epoch 8/10 - Batch 3500/7813 - Loss: 0.0251\n",
      "Epoch 8/10 - Batch 3600/7813 - Loss: 0.0486\n",
      "Epoch 8/10 - Batch 3700/7813 - Loss: 0.0316\n",
      "Epoch 8/10 - Batch 3800/7813 - Loss: 0.0267\n",
      "Epoch 8/10 - Batch 3900/7813 - Loss: 0.0397\n",
      "Epoch 8/10 - Batch 4000/7813 - Loss: 0.0348\n",
      "Epoch 8/10 - Batch 4100/7813 - Loss: 0.0428\n",
      "Epoch 8/10 - Batch 4200/7813 - Loss: 0.0291\n",
      "Epoch 8/10 - Batch 4300/7813 - Loss: 0.0417\n",
      "Epoch 8/10 - Batch 4400/7813 - Loss: 0.0495\n",
      "Epoch 8/10 - Batch 4500/7813 - Loss: 0.0449\n",
      "Epoch 8/10 - Batch 4600/7813 - Loss: 0.0336\n",
      "Epoch 8/10 - Batch 4700/7813 - Loss: 0.0379\n",
      "Epoch 8/10 - Batch 4800/7813 - Loss: 0.0351\n",
      "Epoch 8/10 - Batch 4900/7813 - Loss: 0.0419\n",
      "Epoch 8/10 - Batch 5000/7813 - Loss: 0.0271\n",
      "Epoch 8/10 - Batch 5100/7813 - Loss: 0.0396\n",
      "Epoch 8/10 - Batch 5200/7813 - Loss: 0.0404\n",
      "Epoch 8/10 - Batch 5300/7813 - Loss: 0.0522\n",
      "Epoch 8/10 - Batch 5400/7813 - Loss: 0.0389\n",
      "Epoch 8/10 - Batch 5500/7813 - Loss: 0.0327\n",
      "Epoch 8/10 - Batch 5600/7813 - Loss: 0.0346\n",
      "Epoch 8/10 - Batch 5700/7813 - Loss: 0.0473\n",
      "Epoch 8/10 - Batch 5800/7813 - Loss: 0.0356\n",
      "Epoch 8/10 - Batch 5900/7813 - Loss: 0.0410\n",
      "Epoch 8/10 - Batch 6000/7813 - Loss: 0.0499\n",
      "Epoch 8/10 - Batch 6100/7813 - Loss: 0.0401\n",
      "Epoch 8/10 - Batch 6200/7813 - Loss: 0.0374\n",
      "Epoch 8/10 - Batch 6300/7813 - Loss: 0.0390\n",
      "Epoch 8/10 - Batch 6400/7813 - Loss: 0.0362\n",
      "Epoch 8/10 - Batch 6500/7813 - Loss: 0.0381\n",
      "Epoch 8/10 - Batch 6600/7813 - Loss: 0.0376\n",
      "Epoch 8/10 - Batch 6700/7813 - Loss: 0.0288\n",
      "Epoch 8/10 - Batch 6800/7813 - Loss: 0.0562\n",
      "Epoch 8/10 - Batch 6900/7813 - Loss: 0.0354\n",
      "Epoch 8/10 - Batch 7000/7813 - Loss: 0.0250\n",
      "Epoch 8/10 - Batch 7100/7813 - Loss: 0.0381\n",
      "Epoch 8/10 - Batch 7200/7813 - Loss: 0.0426\n",
      "Epoch 8/10 - Batch 7300/7813 - Loss: 0.0440\n",
      "Epoch 8/10 - Batch 7400/7813 - Loss: 0.0406\n",
      "Epoch 8/10 - Batch 7500/7813 - Loss: 0.0469\n",
      "Epoch 8/10 - Batch 7600/7813 - Loss: 0.0421\n",
      "Epoch 8/10 - Batch 7700/7813 - Loss: 0.0471\n",
      "Epoch 8/10 - Batch 7800/7813 - Loss: 0.0487\n",
      "Epoch 8/10:\n",
      "Training Loss: 0.0375\n",
      "Validation Loss: 0.0466\n",
      "Epoch 9/10 - Batch 0/7813 - Loss: 0.0228\n",
      "Epoch 9/10 - Batch 100/7813 - Loss: 0.0335\n",
      "Epoch 9/10 - Batch 200/7813 - Loss: 0.0269\n",
      "Epoch 9/10 - Batch 300/7813 - Loss: 0.0422\n",
      "Epoch 9/10 - Batch 400/7813 - Loss: 0.0327\n",
      "Epoch 9/10 - Batch 500/7813 - Loss: 0.0301\n",
      "Epoch 9/10 - Batch 600/7813 - Loss: 0.0292\n",
      "Epoch 9/10 - Batch 700/7813 - Loss: 0.0401\n",
      "Epoch 9/10 - Batch 800/7813 - Loss: 0.0547\n",
      "Epoch 9/10 - Batch 900/7813 - Loss: 0.0358\n",
      "Epoch 9/10 - Batch 1000/7813 - Loss: 0.0529\n",
      "Epoch 9/10 - Batch 1100/7813 - Loss: 0.0445\n",
      "Epoch 9/10 - Batch 1200/7813 - Loss: 0.0277\n",
      "Epoch 9/10 - Batch 1300/7813 - Loss: 0.0333\n",
      "Epoch 9/10 - Batch 1400/7813 - Loss: 0.0197\n",
      "Epoch 9/10 - Batch 1500/7813 - Loss: 0.0379\n",
      "Epoch 9/10 - Batch 1600/7813 - Loss: 0.0307\n",
      "Epoch 9/10 - Batch 1700/7813 - Loss: 0.0452\n",
      "Epoch 9/10 - Batch 1800/7813 - Loss: 0.0217\n",
      "Epoch 9/10 - Batch 1900/7813 - Loss: 0.0526\n",
      "Epoch 9/10 - Batch 2000/7813 - Loss: 0.0500\n",
      "Epoch 9/10 - Batch 2100/7813 - Loss: 0.0303\n",
      "Epoch 9/10 - Batch 2200/7813 - Loss: 0.0431\n",
      "Epoch 9/10 - Batch 2300/7813 - Loss: 0.0331\n",
      "Epoch 9/10 - Batch 2400/7813 - Loss: 0.0344\n",
      "Epoch 9/10 - Batch 2500/7813 - Loss: 0.0253\n",
      "Epoch 9/10 - Batch 2600/7813 - Loss: 0.0433\n",
      "Epoch 9/10 - Batch 2700/7813 - Loss: 0.0359\n",
      "Epoch 9/10 - Batch 2800/7813 - Loss: 0.0327\n",
      "Epoch 9/10 - Batch 2900/7813 - Loss: 0.0412\n",
      "Epoch 9/10 - Batch 3000/7813 - Loss: 0.0329\n",
      "Epoch 9/10 - Batch 3100/7813 - Loss: 0.0408\n",
      "Epoch 9/10 - Batch 3200/7813 - Loss: 0.0353\n",
      "Epoch 9/10 - Batch 3300/7813 - Loss: 0.0294\n",
      "Epoch 9/10 - Batch 3400/7813 - Loss: 0.0357\n",
      "Epoch 9/10 - Batch 3500/7813 - Loss: 0.0413\n",
      "Epoch 9/10 - Batch 3600/7813 - Loss: 0.0255\n",
      "Epoch 9/10 - Batch 3700/7813 - Loss: 0.0345\n",
      "Epoch 9/10 - Batch 3800/7813 - Loss: 0.0378\n",
      "Epoch 9/10 - Batch 3900/7813 - Loss: 0.0372\n",
      "Epoch 9/10 - Batch 4000/7813 - Loss: 0.0296\n",
      "Epoch 9/10 - Batch 4100/7813 - Loss: 0.0274\n",
      "Epoch 9/10 - Batch 4200/7813 - Loss: 0.0323\n",
      "Epoch 9/10 - Batch 4300/7813 - Loss: 0.0361\n",
      "Epoch 9/10 - Batch 4400/7813 - Loss: 0.0314\n",
      "Epoch 9/10 - Batch 4500/7813 - Loss: 0.0442\n",
      "Epoch 9/10 - Batch 4600/7813 - Loss: 0.0440\n",
      "Epoch 9/10 - Batch 4700/7813 - Loss: 0.0405\n",
      "Epoch 9/10 - Batch 4800/7813 - Loss: 0.0353\n",
      "Epoch 9/10 - Batch 4900/7813 - Loss: 0.0297\n",
      "Epoch 9/10 - Batch 5000/7813 - Loss: 0.0427\n",
      "Epoch 9/10 - Batch 5100/7813 - Loss: 0.0424\n",
      "Epoch 9/10 - Batch 5200/7813 - Loss: 0.0460\n",
      "Epoch 9/10 - Batch 5300/7813 - Loss: 0.0590\n",
      "Epoch 9/10 - Batch 5400/7813 - Loss: 0.0303\n",
      "Epoch 9/10 - Batch 5500/7813 - Loss: 0.0334\n",
      "Epoch 9/10 - Batch 5600/7813 - Loss: 0.0578\n",
      "Epoch 9/10 - Batch 5700/7813 - Loss: 0.0385\n",
      "Epoch 9/10 - Batch 5800/7813 - Loss: 0.0343\n",
      "Epoch 9/10 - Batch 5900/7813 - Loss: 0.0543\n",
      "Epoch 9/10 - Batch 6000/7813 - Loss: 0.0404\n",
      "Epoch 9/10 - Batch 6100/7813 - Loss: 0.0253\n",
      "Epoch 9/10 - Batch 6200/7813 - Loss: 0.0433\n",
      "Epoch 9/10 - Batch 6300/7813 - Loss: 0.0384\n",
      "Epoch 9/10 - Batch 6400/7813 - Loss: 0.0304\n",
      "Epoch 9/10 - Batch 6500/7813 - Loss: 0.0340\n",
      "Epoch 9/10 - Batch 6600/7813 - Loss: 0.0428\n",
      "Epoch 9/10 - Batch 6700/7813 - Loss: 0.0142\n",
      "Epoch 9/10 - Batch 6800/7813 - Loss: 0.0390\n",
      "Epoch 9/10 - Batch 6900/7813 - Loss: 0.0318\n",
      "Epoch 9/10 - Batch 7000/7813 - Loss: 0.0280\n",
      "Epoch 9/10 - Batch 7100/7813 - Loss: 0.0246\n",
      "Epoch 9/10 - Batch 7200/7813 - Loss: 0.0374\n",
      "Epoch 9/10 - Batch 7300/7813 - Loss: 0.0287\n",
      "Epoch 9/10 - Batch 7400/7813 - Loss: 0.0338\n",
      "Epoch 9/10 - Batch 7500/7813 - Loss: 0.0483\n",
      "Epoch 9/10 - Batch 7600/7813 - Loss: 0.0472\n",
      "Epoch 9/10 - Batch 7700/7813 - Loss: 0.0394\n",
      "Epoch 9/10 - Batch 7800/7813 - Loss: 0.0294\n",
      "Epoch 9/10:\n",
      "Training Loss: 0.0375\n",
      "Validation Loss: 0.0466\n",
      "Epoch 10/10 - Batch 0/7813 - Loss: 0.0342\n",
      "Epoch 10/10 - Batch 100/7813 - Loss: 0.0204\n",
      "Epoch 10/10 - Batch 200/7813 - Loss: 0.0202\n",
      "Epoch 10/10 - Batch 300/7813 - Loss: 0.0295\n",
      "Epoch 10/10 - Batch 400/7813 - Loss: 0.0328\n",
      "Epoch 10/10 - Batch 500/7813 - Loss: 0.0237\n",
      "Epoch 10/10 - Batch 600/7813 - Loss: 0.0240\n",
      "Epoch 10/10 - Batch 700/7813 - Loss: 0.0252\n",
      "Epoch 10/10 - Batch 800/7813 - Loss: 0.0368\n",
      "Epoch 10/10 - Batch 900/7813 - Loss: 0.0265\n",
      "Epoch 10/10 - Batch 1000/7813 - Loss: 0.0314\n",
      "Epoch 10/10 - Batch 1100/7813 - Loss: 0.0284\n",
      "Epoch 10/10 - Batch 1200/7813 - Loss: 0.0264\n",
      "Epoch 10/10 - Batch 1300/7813 - Loss: 0.0505\n",
      "Epoch 10/10 - Batch 1400/7813 - Loss: 0.0402\n",
      "Epoch 10/10 - Batch 1500/7813 - Loss: 0.0227\n",
      "Epoch 10/10 - Batch 1600/7813 - Loss: 0.0420\n",
      "Epoch 10/10 - Batch 1700/7813 - Loss: 0.0348\n",
      "Epoch 10/10 - Batch 1800/7813 - Loss: 0.0188\n",
      "Epoch 10/10 - Batch 1900/7813 - Loss: 0.0264\n",
      "Epoch 10/10 - Batch 2000/7813 - Loss: 0.0180\n",
      "Epoch 10/10 - Batch 2100/7813 - Loss: 0.0270\n",
      "Epoch 10/10 - Batch 2200/7813 - Loss: 0.0170\n",
      "Epoch 10/10 - Batch 2300/7813 - Loss: 0.0255\n",
      "Epoch 10/10 - Batch 2400/7813 - Loss: 0.0259\n",
      "Epoch 10/10 - Batch 2500/7813 - Loss: 0.0460\n",
      "Epoch 10/10 - Batch 2600/7813 - Loss: 0.0328\n",
      "Epoch 10/10 - Batch 2700/7813 - Loss: 0.0278\n",
      "Epoch 10/10 - Batch 2800/7813 - Loss: 0.0425\n",
      "Epoch 10/10 - Batch 2900/7813 - Loss: 0.0296\n",
      "Epoch 10/10 - Batch 3000/7813 - Loss: 0.0347\n",
      "Epoch 10/10 - Batch 3100/7813 - Loss: 0.0217\n",
      "Epoch 10/10 - Batch 3200/7813 - Loss: 0.0489\n",
      "Epoch 10/10 - Batch 3300/7813 - Loss: 0.0296\n",
      "Epoch 10/10 - Batch 3400/7813 - Loss: 0.0342\n",
      "Epoch 10/10 - Batch 3500/7813 - Loss: 0.0373\n",
      "Epoch 10/10 - Batch 3600/7813 - Loss: 0.0391\n",
      "Epoch 10/10 - Batch 3700/7813 - Loss: 0.0254\n",
      "Epoch 10/10 - Batch 3800/7813 - Loss: 0.0277\n",
      "Epoch 10/10 - Batch 3900/7813 - Loss: 0.0448\n",
      "Epoch 10/10 - Batch 4000/7813 - Loss: 0.0437\n",
      "Epoch 10/10 - Batch 4100/7813 - Loss: 0.0250\n",
      "Epoch 10/10 - Batch 4200/7813 - Loss: 0.0323\n",
      "Epoch 10/10 - Batch 4300/7813 - Loss: 0.0399\n",
      "Epoch 10/10 - Batch 4400/7813 - Loss: 0.0228\n",
      "Epoch 10/10 - Batch 4500/7813 - Loss: 0.0300\n",
      "Epoch 10/10 - Batch 4600/7813 - Loss: 0.0220\n",
      "Epoch 10/10 - Batch 4700/7813 - Loss: 0.0495\n",
      "Epoch 10/10 - Batch 4800/7813 - Loss: 0.0250\n",
      "Epoch 10/10 - Batch 4900/7813 - Loss: 0.0210\n",
      "Epoch 10/10 - Batch 5000/7813 - Loss: 0.0357\n",
      "Epoch 10/10 - Batch 5100/7813 - Loss: 0.0272\n",
      "Epoch 10/10 - Batch 5200/7813 - Loss: 0.0330\n",
      "Epoch 10/10 - Batch 5300/7813 - Loss: 0.0399\n",
      "Epoch 10/10 - Batch 5400/7813 - Loss: 0.0188\n",
      "Epoch 10/10 - Batch 5500/7813 - Loss: 0.0449\n",
      "Epoch 10/10 - Batch 5600/7813 - Loss: 0.0350\n",
      "Epoch 10/10 - Batch 5700/7813 - Loss: 0.0303\n",
      "Epoch 10/10 - Batch 5800/7813 - Loss: 0.0304\n",
      "Epoch 10/10 - Batch 5900/7813 - Loss: 0.0409\n",
      "Epoch 10/10 - Batch 6000/7813 - Loss: 0.0280\n",
      "Epoch 10/10 - Batch 6100/7813 - Loss: 0.0284\n",
      "Epoch 10/10 - Batch 6200/7813 - Loss: 0.0260\n",
      "Epoch 10/10 - Batch 6300/7813 - Loss: 0.0417\n",
      "Epoch 10/10 - Batch 6400/7813 - Loss: 0.0329\n",
      "Epoch 10/10 - Batch 6500/7813 - Loss: 0.0324\n",
      "Epoch 10/10 - Batch 6600/7813 - Loss: 0.0378\n",
      "Epoch 10/10 - Batch 6700/7813 - Loss: 0.0392\n",
      "Epoch 10/10 - Batch 6800/7813 - Loss: 0.0314\n",
      "Epoch 10/10 - Batch 6900/7813 - Loss: 0.0441\n",
      "Epoch 10/10 - Batch 7000/7813 - Loss: 0.0315\n",
      "Epoch 10/10 - Batch 7100/7813 - Loss: 0.0271\n",
      "Epoch 10/10 - Batch 7200/7813 - Loss: 0.0237\n",
      "Epoch 10/10 - Batch 7300/7813 - Loss: 0.0405\n",
      "Epoch 10/10 - Batch 7400/7813 - Loss: 0.0542\n",
      "Epoch 10/10 - Batch 7500/7813 - Loss: 0.0336\n",
      "Epoch 10/10 - Batch 7600/7813 - Loss: 0.0273\n",
      "Epoch 10/10 - Batch 7700/7813 - Loss: 0.0318\n",
      "Epoch 10/10 - Batch 7800/7813 - Loss: 0.0326\n",
      "Epoch 10/10:\n",
      "Training Loss: 0.0322\n",
      "Validation Loss: 0.0484\n"
     ]
    }
   ],
   "source": [
    "def prepare_and_train():\n",
    "    num_users = len(user_mapping)\n",
    "    num_items = len(item_mapping)\n",
    "\n",
    "    # Create train/val datasets\n",
    "    train_dataset = PratilipiDataset(\n",
    "        user_indices[:train_size],\n",
    "        item_indices[:train_size],\n",
    "        ratings[:train_size]\n",
    "    )\n",
    "    val_dataset = PratilipiDataset(\n",
    "        user_indices[train_size:],\n",
    "        item_indices[train_size:],\n",
    "        ratings[train_size:]\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=256)\n",
    "\n",
    "    model = NCF(num_users, num_items)\n",
    "    NCF.forward = forward\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_model(model, train_loader, val_loader, device=device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = prepare_and_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_model(model, user_id, num_recommendations=5):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "    with torch.no_grad():\n",
    "        # Get user index from mapping\n",
    "        user_idx = user_mapping[user_id]\n",
    "        user_tensor = torch.LongTensor([user_idx]).to(device)\n",
    "\n",
    "        # Create tensors for all items\n",
    "        all_items = torch.LongTensor(list(item_mapping.values())).to(device)\n",
    "\n",
    "        # Expand user tensor to match items dimension\n",
    "        user_tensor = user_tensor.expand(len(all_items))\n",
    "\n",
    "        # Get predictions for all items\n",
    "        predictions = model(user_tensor, all_items)\n",
    "\n",
    "        # Get top N recommendations\n",
    "        top_n = torch.topk(predictions, num_recommendations)\n",
    "\n",
    "        # Convert indices back to pratilipi_ids\n",
    "        reverse_item_mapping = {v: k for k, v in item_mapping.items()}\n",
    "        recommended_items = [reverse_item_mapping[idx.item()]\n",
    "                             for idx in top_n.indices]\n",
    "\n",
    "        return recommended_items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_recommendations():\n",
    "    # Load the best saved model\n",
    "    model = NCF(num_users=len(user_mapping), num_items=len(item_mapping))\n",
    "    model.load_state_dict(torch.load('best_model.pt'))\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Test for a specific user\n",
    "    test_user_id = list(user_mapping.keys())[0]  # Get first user as example\n",
    "    recommendations = test_model(model, test_user_id)\n",
    "\n",
    "    print(f\"Recommendations for user {test_user_id}:\")\n",
    "    for i, item_id in enumerate(recommendations, 1):\n",
    "        print(f\"{i}. Pratilipi ID: {item_id}\")\n",
    "\n",
    "    return recommendations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate evaluation metrics\n",
    "def calculate_metrics(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    predictions_list = []\n",
    "    actuals_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for users, items, ratings in test_loader:\n",
    "            users = users.to(device)\n",
    "            items = items.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "\n",
    "            predictions = model(users, items)\n",
    "\n",
    "            predictions_list.extend(predictions.cpu().numpy())\n",
    "            actuals_list.extend(ratings.cpu().numpy())\n",
    "\n",
    "    # Calculate MSE\n",
    "    mse = np.mean((np.array(predictions_list) - np.array(actuals_list)) ** 2)\n",
    "    # Calculate RMSE\n",
    "    rmse = np.sqrt(mse)\n",
    "    # Calculate MAE\n",
    "    mae = np.mean(np.abs(np.array(predictions_list) - np.array(actuals_list)))\n",
    "\n",
    "    return {\n",
    "        'MSE': mse,\n",
    "        'RMSE': rmse,\n",
    "        'MAE': mae\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_and_evaluate():\n",
    "    # Load model\n",
    "    model = NCF(num_users=len(user_mapping), num_items=len(item_mapping))\n",
    "    model.load_state_dict(torch.load('best_model.pt'))\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = model.to(device)\n",
    "\n",
    "    # Generate recommendations\n",
    "    recommendations = generate_recommendations()\n",
    "\n",
    "    # Create test dataset and loader\n",
    "    test_dataset = PratilipiDataset(\n",
    "        user_indices[train_size:],\n",
    "        item_indices[train_size:],\n",
    "        ratings[train_size:]\n",
    "    )\n",
    "    test_loader = DataLoader(test_dataset, batch_size=256)\n",
    "\n",
    "    # Calculate metrics\n",
    "    metrics = calculate_metrics(model, test_loader, device)\n",
    "\n",
    "    print(\"\\nModel Performance Metrics:\")\n",
    "    for metric, value in metrics.items():\n",
    "        print(f\"{metric}: {value:.4f}\")\n",
    "\n",
    "    return recommendations, metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Recommendations for user 5506791954036110:\n",
      "1. Pratilipi ID: 1377786228274181\n",
      "2. Pratilipi ID: 1377786228254759\n",
      "3. Pratilipi ID: 1377786228240538\n",
      "4. Pratilipi ID: 1377786228333808\n",
      "5. Pratilipi ID: 1377786228214640\n",
      "\n",
      "Model Performance Metrics:\n",
      "MSE: 0.0465\n",
      "RMSE: 0.2156\n",
      "MAE: 0.1136\n"
     ]
    }
   ],
   "source": [
    "recommendations, metrics = test_and_evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model and mappings saved successfully!\n"
     ]
    }
   ],
   "source": [
    "def save_model_files(model, user_mapping, item_mapping):\n",
    "    # Save model\n",
    "    torch.save(model.state_dict(), 'best_model.pt')\n",
    "\n",
    "    # Save mappings\n",
    "    torch.save(user_mapping, 'user_mapping.pt')\n",
    "    torch.save(item_mapping, 'item_mapping.pt')\n",
    "\n",
    "    print(\"Model and mappings saved successfully!\")\n",
    "\n",
    "\n",
    "# Call this after training\n",
    "save_model_files(model, user_mapping, item_mapping)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting implicit\n",
      "  Downloading implicit-0.7.2-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.26.2)\n",
      "Requirement already satisfied: scipy>=0.16 in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from implicit) (1.15.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from implicit) (4.66.1)\n",
      "Requirement already satisfied: threadpoolctl in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from implicit) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sayan\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sayan\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->implicit) (0.4.4)\n",
      "Downloading implicit-0.7.2-cp310-cp310-win_amd64.whl (748 kB)\n",
      "   ---------------------------------------- 0.0/748.6 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 262.1/748.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 748.6/748.6 kB 2.1 MB/s eta 0:00:00\n",
      "Installing collected packages: implicit\n",
      "Successfully installed implicit-0.7.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install implicit scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(user_interactions_path, meta_data_path):\n",
    "    interactions_df = pd.read_csv(user_interactions_path)\n",
    "    meta_df = pd.read_csv(meta_data_path)\n",
    "    \n",
    "    # Convert timestamps\n",
    "    interactions_df['updated_at'] = pd.to_datetime(interactions_df['updated_at'])\n",
    "    meta_df['updated_at'] = pd.to_datetime(meta_df['updated_at'])\n",
    "    meta_df['published_at'] = pd.to_datetime(meta_df['published_at'])\n",
    "    \n",
    "    interactions_df = interactions_df.sort_values('updated_at')\n",
    "    \n",
    "    return interactions_df, meta_df\n",
    "\n",
    "interactions_df, meta_df = load_and_preprocess_data('user_interaction.csv', 'metadata.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Interactions:\n",
      "                  user_id      pratilipi_id  read_percent  \\\n",
      "1033131  5506791954036110  1377786225804654         100.0   \n",
      "1415300  5506791980439899  1377786228150074         100.0   \n",
      "2318259  5506791979182708  1377786218415632         100.0   \n",
      "952322   5506791996330389  1377786219497547         100.0   \n",
      "2114134  5506791961370166  1377786224952303         100.0   \n",
      "\n",
      "                     updated_at  \n",
      "1033131 2022-03-18 15:14:41.827  \n",
      "1415300 2022-03-18 15:14:42.120  \n",
      "2318259 2022-03-18 15:14:42.134  \n",
      "952322  2022-03-18 15:14:42.170  \n",
      "2114134 2022-03-18 15:14:42.282  \n",
      "\n",
      "Meta Data:\n",
      "          author_id      pratilipi_id category_name  reading_time  \\\n",
      "0 -3418949279741297  1025741862639304   translation             0   \n",
      "1 -2270332351871840  1377786215601277   translation           171   \n",
      "2 -2270332352037261  1377786215601962   translation            92   \n",
      "3 -2270332352521845  1377786215640994   translation             0   \n",
      "4 -2270332349665658  1377786215931338   translation            47   \n",
      "\n",
      "           updated_at        published_at  \n",
      "0 2020-08-19 15:26:13 2016-09-30 10:37:04  \n",
      "1 2021-01-21 16:27:07 2018-06-11 13:17:48  \n",
      "2 2020-09-29 12:33:57 2018-06-12 04:19:12  \n",
      "3 2019-10-17 09:03:37 2019-09-26 14:58:53  \n",
      "4 2020-05-05 11:33:41 2018-11-25 12:28:23  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"User Interactions:\")\n",
    "print(interactions_df.head())\n",
    "print(\"\\nMeta Data:\")\n",
    "print(meta_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data structure:\n",
      "\n",
      "Columns: ['user_id', 'pratilipi_id', 'read_percent', 'updated_at']\n",
      "\n",
      "Sample of data:\n",
      "                  user_id      pratilipi_id  read_percent  \\\n",
      "1033131  5506791954036110  1377786225804654         100.0   \n",
      "1415300  5506791980439899  1377786228150074         100.0   \n",
      "2318259  5506791979182708  1377786218415632         100.0   \n",
      "952322   5506791996330389  1377786219497547         100.0   \n",
      "2114134  5506791961370166  1377786224952303         100.0   \n",
      "\n",
      "                     updated_at  \n",
      "1033131 2022-03-18 15:14:41.827  \n",
      "1415300 2022-03-18 15:14:42.120  \n",
      "2318259 2022-03-18 15:14:42.134  \n",
      "952322  2022-03-18 15:14:42.170  \n",
      "2114134 2022-03-18 15:14:42.282  \n",
      "\n",
      "Data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2500000 entries, 1033131 to 2186369\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Dtype         \n",
      "---  ------        -----         \n",
      " 0   user_id       int64         \n",
      " 1   pratilipi_id  int64         \n",
      " 2   read_percent  float64       \n",
      " 3   updated_at    datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(1), int64(2)\n",
      "memory usage: 95.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking data structure:\")\n",
    "print(\"\\nColumns:\", interactions_df.columns.tolist())\n",
    "print(\"\\nSample of data:\")\n",
    "print(interactions_df.head())\n",
    "print(\"\\nData info:\")\n",
    "print(interactions_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "user_mapping = {id: i for i, id in enumerate(\n",
    "    interactions_df['user_id'].unique())}\n",
    "item_mapping = {id: i for i, id in enumerate(\n",
    "    interactions_df['pratilipi_id'].unique())}\n",
    "\n",
    "# 2. Create the necessary variables for sparse matrix\n",
    "user_indices = [user_mapping[user] for user in interactions_df['user_id']]\n",
    "item_indices = [item_mapping[item] for item in interactions_df['pratilipi_id']]\n",
    "# Using read_percent as ratings\n",
    "ratings = interactions_df['read_percent'].values\n",
    "\n",
    "# 3. Get dimensions for the matrix\n",
    "n_users = len(user_mapping)\n",
    "n_items = len(item_mapping)\n",
    "\n",
    "# 4. Now create the sparse matrix\n",
    "sparse_matrix = csr_matrix((ratings, (user_indices, item_indices)),\n",
    "                           shape=(n_users, n_items),\n",
    "                           dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def split_train_test(sparse_matrix, test_size=0.25, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the sparse interaction matrix into training and test sets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sparse_matrix : scipy.sparse.csr_matrix\n",
    "        The user-item interaction matrix\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing (default: 0.25 as per README)\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    train_matrix : scipy.sparse.csr_matrix\n",
    "        Training set matrix\n",
    "    test_matrix : scipy.sparse.csr_matrix\n",
    "        Test set matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # Get indices of all non-zero elements\n",
    "    nonzero = sparse_matrix.nonzero()\n",
    "    indices = list(zip(nonzero[0], nonzero[1]))\n",
    "    values = sparse_matrix.data\n",
    "\n",
    "    # Split the indices\n",
    "    train_indices, test_indices = train_test_split(\n",
    "        range(len(indices)),\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Create training matrix\n",
    "    train_matrix = sparse_matrix.copy()\n",
    "    train_matrix.data = np.zeros_like(train_matrix.data)\n",
    "    for idx in train_indices:\n",
    "        row, col = indices[idx]\n",
    "        train_matrix[row, col] = values[idx]\n",
    "\n",
    "    # Create test matrix\n",
    "    test_matrix = sparse_matrix.copy()\n",
    "    test_matrix.data = np.zeros_like(test_matrix.data)\n",
    "    for idx in test_indices:\n",
    "        row, col = indices[idx]\n",
    "        test_matrix[row, col] = values[idx]\n",
    "\n",
    "    # Convert to CSR format for efficiency\n",
    "    train_matrix = train_matrix.tocsr()\n",
    "    test_matrix = test_matrix.tocsr()\n",
    "\n",
    "    return train_matrix, test_matrix\n",
    "\n",
    "\n",
    "# Usage:\n",
    "train_matrix, test_matrix = split_train_test(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix density: 0.000043\n",
      "Training matrix density: 0.000043\n",
      "Test matrix density: 0.000043\n"
     ]
    }
   ],
   "source": [
    "# Print statistics about the split\n",
    "print(\n",
    "    f\"Original matrix density: {sparse_matrix.nnz / (sparse_matrix.shape[0] * sparse_matrix.shape[1]):.6f}\")\n",
    "print(\n",
    "    f\"Training matrix density: {train_matrix.nnz / (train_matrix.shape[0] * train_matrix.shape[1]):.6f}\")\n",
    "print(\n",
    "    f\"Test matrix density: {test_matrix.nnz / (test_matrix.shape[0] * test_matrix.shape[1]):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PratilipiDataset(Dataset):\n",
    "    def __init__(self, user_indices, item_indices, ratings):\n",
    "        self.user_indices = torch.LongTensor(user_indices)\n",
    "        self.item_indices = torch.LongTensor(item_indices)\n",
    "        # Normalize ratings to 0-1 range\n",
    "        self.ratings = torch.FloatTensor(ratings) / 100.0  # Since read_percent is 0-100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=50):\n",
    "        super(NCF, self).__init__()\n",
    "\n",
    "        # Embedding layers\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "        # MLP layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(2 * embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, user_input, item_input):\n",
    "    # Get embeddings\n",
    "    user_embedded = self.user_embedding(user_input)\n",
    "    item_embedded = self.item_embedding(item_input)\n",
    "\n",
    "    # Concatenate embeddings\n",
    "    vector = torch.cat([user_embedded, item_embedded], dim=-1)\n",
    "\n",
    "    # Pass through MLP\n",
    "    prediction = self.fc_layers(vector)\n",
    "    return prediction.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_model(model, test_loader, device):\n",
    "    model.eval()\n",
    "    total_loss = 0\n",
    "    criterion = nn.MSELoss()\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for users, items, ratings in test_loader:\n",
    "            users = users.to(device)\n",
    "            items = items.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "\n",
    "            predictions = model(users, items)\n",
    "            loss = criterion(predictions, ratings)\n",
    "            total_loss += loss.item()\n",
    "\n",
    "    return total_loss / len(test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'PratilipiDataset' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[37], line 14\u001b[0m\n\u001b[0;32m      3\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m PratilipiDataset(\n\u001b[0;32m      4\u001b[0m     user_indices[:train_size],\n\u001b[0;32m      5\u001b[0m     item_indices[:train_size],\n\u001b[0;32m      6\u001b[0m     ratings[:train_size]\n\u001b[0;32m      7\u001b[0m )\n\u001b[0;32m      8\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m PratilipiDataset(\n\u001b[0;32m      9\u001b[0m     user_indices[train_size:],\n\u001b[0;32m     10\u001b[0m     item_indices[train_size:],\n\u001b[0;32m     11\u001b[0m     ratings[train_size:]\n\u001b[0;32m     12\u001b[0m )\n\u001b[1;32m---> 14\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     15\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Sayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:350\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 350\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\sampler.py:142\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement, \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_samples\u001b[49m, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Sayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\sampler.py:149\u001b[0m, in \u001b[0;36mRandomSampler.num_samples\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnum_samples\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;66;03m# dataset size might change at runtime\u001b[39;00m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 149\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_samples\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'PratilipiDataset' has no len()"
     ]
    }
   ],
   "source": [
    "# Split data into train and validation\n",
    "train_size = int(0.8 * len(user_indices))\n",
    "train_dataset = PratilipiDataset(\n",
    "    user_indices[:train_size],\n",
    "    item_indices[:train_size],\n",
    "    ratings[:train_size]\n",
    ")\n",
    "val_dataset = PratilipiDataset(\n",
    "    user_indices[train_size:],\n",
    "    item_indices[train_size:],\n",
    "    ratings[train_size:]\n",
    ")\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=256)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, val_loader, epochs=10, lr=0.001, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, weight_decay=1e-5)\n",
    "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
    "        optimizer, 'min', patience=2)\n",
    "\n",
    "    best_val_loss = float('inf')\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        # Training\n",
    "        model.train()\n",
    "        train_loss = 0\n",
    "        for batch_idx, (users, items, ratings) in enumerate(train_loader):\n",
    "            users = users.to(device)\n",
    "            items = items.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "\n",
    "            predictions = model(users, items)\n",
    "            loss = criterion(predictions, ratings)\n",
    "\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
    "            optimizer.step()\n",
    "\n",
    "            train_loss += loss.item()\n",
    "\n",
    "            if batch_idx % 100 == 0:\n",
    "                print(\n",
    "                    f'Epoch {epoch+1}/{epochs} - Batch {batch_idx}/{len(train_loader)} - Loss: {loss.item():.4f}')\n",
    "\n",
    "        # Validation\n",
    "        val_loss = evaluate_model(model, val_loader, device)\n",
    "        scheduler.step(val_loss)\n",
    "\n",
    "        print(f'Epoch {epoch+1}/{epochs}:')\n",
    "        print(f'Training Loss: {train_loss/len(train_loader):.4f}')\n",
    "        print(f'Validation Loss: {val_loss:.4f}')\n",
    "\n",
    "        # Save best model\n",
    "        if val_loss < best_val_loss:\n",
    "            best_val_loss = val_loss\n",
    "            torch.save(model.state_dict(), 'best_model.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "object of type 'PratilipiDataset' has no len()",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[39], line 29\u001b[0m\n\u001b[0;32m     24\u001b[0m     train_model(model, train_loader, val_loader, device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     26\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m model\n\u001b[1;32m---> 29\u001b[0m model \u001b[38;5;241m=\u001b[39m \u001b[43mprepare_and_train\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[39], line 17\u001b[0m, in \u001b[0;36mprepare_and_train\u001b[1;34m()\u001b[0m\n\u001b[0;32m      6\u001b[0m train_dataset \u001b[38;5;241m=\u001b[39m PratilipiDataset(\n\u001b[0;32m      7\u001b[0m     user_indices[:train_size],\n\u001b[0;32m      8\u001b[0m     item_indices[:train_size],\n\u001b[0;32m      9\u001b[0m     ratings[:train_size]\n\u001b[0;32m     10\u001b[0m )\n\u001b[0;32m     11\u001b[0m val_dataset \u001b[38;5;241m=\u001b[39m PratilipiDataset(\n\u001b[0;32m     12\u001b[0m     user_indices[train_size:],\n\u001b[0;32m     13\u001b[0m     item_indices[train_size:],\n\u001b[0;32m     14\u001b[0m     ratings[train_size:]\n\u001b[0;32m     15\u001b[0m )\n\u001b[1;32m---> 17\u001b[0m train_loader \u001b[38;5;241m=\u001b[39m \u001b[43mDataLoader\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_dataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m256\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mshuffle\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[0;32m     18\u001b[0m val_loader \u001b[38;5;241m=\u001b[39m DataLoader(val_dataset, batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m256\u001b[39m)\n\u001b[0;32m     20\u001b[0m model \u001b[38;5;241m=\u001b[39m NCF(num_users, num_items)\n",
      "File \u001b[1;32mc:\\Users\\Sayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\dataloader.py:350\u001b[0m, in \u001b[0;36mDataLoader.__init__\u001b[1;34m(self, dataset, batch_size, shuffle, sampler, batch_sampler, num_workers, collate_fn, pin_memory, drop_last, timeout, worker_init_fn, multiprocessing_context, generator, prefetch_factor, persistent_workers, pin_memory_device)\u001b[0m\n\u001b[0;32m    348\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:  \u001b[38;5;66;03m# map-style\u001b[39;00m\n\u001b[0;32m    349\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m shuffle:\n\u001b[1;32m--> 350\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m \u001b[43mRandomSampler\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgenerator\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgenerator\u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[0;32m    351\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    352\u001b[0m         sampler \u001b[38;5;241m=\u001b[39m SequentialSampler(dataset)  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\Sayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\sampler.py:142\u001b[0m, in \u001b[0;36mRandomSampler.__init__\u001b[1;34m(self, data_source, replacement, num_samples, generator)\u001b[0m\n\u001b[0;32m    139\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement, \u001b[38;5;28mbool\u001b[39m):\n\u001b[0;32m    140\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mreplacement should be a boolean value, but got replacement=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mreplacement\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m--> 142\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mnum_samples\u001b[49m, \u001b[38;5;28mint\u001b[39m) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples \u001b[38;5;241m<\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m    143\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnum_samples should be a positive integer value, but got num_samples=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mnum_samples\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\Sayan\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\torch\\utils\\data\\sampler.py:149\u001b[0m, in \u001b[0;36mRandomSampler.num_samples\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    145\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[0;32m    146\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mnum_samples\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28mint\u001b[39m:\n\u001b[0;32m    147\u001b[0m     \u001b[38;5;66;03m# dataset size might change at runtime\u001b[39;00m\n\u001b[0;32m    148\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_samples \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 149\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mlen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_source\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    150\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_num_samples\n",
      "\u001b[1;31mTypeError\u001b[0m: object of type 'PratilipiDataset' has no len()"
     ]
    }
   ],
   "source": [
    "def prepare_and_train():\n",
    "    num_users = len(user_mapping)\n",
    "    num_items = len(item_mapping)\n",
    "\n",
    "    # Create train/val datasets\n",
    "    train_dataset = PratilipiDataset(\n",
    "        user_indices[:train_size],\n",
    "        item_indices[:train_size],\n",
    "        ratings[:train_size]\n",
    "    )\n",
    "    val_dataset = PratilipiDataset(\n",
    "        user_indices[train_size:],\n",
    "        item_indices[train_size:],\n",
    "        ratings[train_size:]\n",
    "    )\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=256, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=256)\n",
    "\n",
    "    model = NCF(num_users, num_items)\n",
    "    NCF.forward = forward\n",
    "\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_model(model, train_loader, val_loader, device=device)\n",
    "\n",
    "    return model\n",
    "\n",
    "\n",
    "model = prepare_and_train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

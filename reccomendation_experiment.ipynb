{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting implicit\n",
      "  Downloading implicit-0.7.2-cp310-cp310-win_amd64.whl.metadata (6.3 kB)\n",
      "Requirement already satisfied: scikit-learn in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.5.2)\n",
      "Requirement already satisfied: pandas in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (2.0.2)\n",
      "Requirement already satisfied: numpy in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (1.26.2)\n",
      "Requirement already satisfied: scipy>=0.16 in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from implicit) (1.15.1)\n",
      "Requirement already satisfied: tqdm>=4.27 in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from implicit) (4.66.1)\n",
      "Requirement already satisfied: threadpoolctl in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from implicit) (3.5.0)\n",
      "Requirement already satisfied: joblib>=1.2.0 in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from scikit-learn) (1.4.2)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in c:\\users\\sayan\\appdata\\roaming\\python\\python310\\site-packages (from pandas) (2.8.2)\n",
      "Requirement already satisfied: pytz>=2020.1 in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: tzdata>=2022.1 in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from pandas) (2023.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\users\\sayan\\appdata\\roaming\\python\\python310\\site-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n",
      "Requirement already satisfied: colorama in c:\\users\\sayan\\appdata\\local\\programs\\python\\python310\\lib\\site-packages (from tqdm>=4.27->implicit) (0.4.4)\n",
      "Downloading implicit-0.7.2-cp310-cp310-win_amd64.whl (748 kB)\n",
      "   ---------------------------------------- 0.0/748.6 kB ? eta -:--:--\n",
      "   -------------- ------------------------- 262.1/748.6 kB ? eta -:--:--\n",
      "   ---------------------------------------- 748.6/748.6 kB 2.1 MB/s eta 0:00:00\n",
      "Installing collected packages: implicit\n",
      "Successfully installed implicit-0.7.2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "\n",
    "!pip install implicit scikit-learn pandas numpy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from scipy.sparse import csr_matrix\n",
    "from implicit.als import AlternatingLeastSquares\n",
    "from datetime import datetime\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_preprocess_data(user_interactions_path, meta_data_path):\n",
    "    interactions_df = pd.read_csv(user_interactions_path)\n",
    "    meta_df = pd.read_csv(meta_data_path)\n",
    "    \n",
    "    # Convert timestamps\n",
    "    interactions_df['updated_at'] = pd.to_datetime(interactions_df['updated_at'])\n",
    "    meta_df['updated_at'] = pd.to_datetime(meta_df['updated_at'])\n",
    "    meta_df['published_at'] = pd.to_datetime(meta_df['published_at'])\n",
    "    \n",
    "    interactions_df = interactions_df.sort_values('updated_at')\n",
    "    \n",
    "    return interactions_df, meta_df\n",
    "\n",
    "interactions_df, meta_df = load_and_preprocess_data('user_interaction.csv', 'metadata.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "User Interactions:\n",
      "                  user_id      pratilipi_id  read_percent  \\\n",
      "1033131  5506791954036110  1377786225804654         100.0   \n",
      "1415300  5506791980439899  1377786228150074         100.0   \n",
      "2318259  5506791979182708  1377786218415632         100.0   \n",
      "952322   5506791996330389  1377786219497547         100.0   \n",
      "2114134  5506791961370166  1377786224952303         100.0   \n",
      "\n",
      "                     updated_at  \n",
      "1033131 2022-03-18 15:14:41.827  \n",
      "1415300 2022-03-18 15:14:42.120  \n",
      "2318259 2022-03-18 15:14:42.134  \n",
      "952322  2022-03-18 15:14:42.170  \n",
      "2114134 2022-03-18 15:14:42.282  \n",
      "\n",
      "Meta Data:\n",
      "          author_id      pratilipi_id category_name  reading_time  \\\n",
      "0 -3418949279741297  1025741862639304   translation             0   \n",
      "1 -2270332351871840  1377786215601277   translation           171   \n",
      "2 -2270332352037261  1377786215601962   translation            92   \n",
      "3 -2270332352521845  1377786215640994   translation             0   \n",
      "4 -2270332349665658  1377786215931338   translation            47   \n",
      "\n",
      "           updated_at        published_at  \n",
      "0 2020-08-19 15:26:13 2016-09-30 10:37:04  \n",
      "1 2021-01-21 16:27:07 2018-06-11 13:17:48  \n",
      "2 2020-09-29 12:33:57 2018-06-12 04:19:12  \n",
      "3 2019-10-17 09:03:37 2019-09-26 14:58:53  \n",
      "4 2020-05-05 11:33:41 2018-11-25 12:28:23  \n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"User Interactions:\")\n",
    "print(interactions_df.head())\n",
    "print(\"\\nMeta Data:\")\n",
    "print(meta_df.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking data structure:\n",
      "\n",
      "Columns: ['user_id', 'pratilipi_id', 'read_percent', 'updated_at']\n",
      "\n",
      "Sample of data:\n",
      "                  user_id      pratilipi_id  read_percent  \\\n",
      "1033131  5506791954036110  1377786225804654         100.0   \n",
      "1415300  5506791980439899  1377786228150074         100.0   \n",
      "2318259  5506791979182708  1377786218415632         100.0   \n",
      "952322   5506791996330389  1377786219497547         100.0   \n",
      "2114134  5506791961370166  1377786224952303         100.0   \n",
      "\n",
      "                     updated_at  \n",
      "1033131 2022-03-18 15:14:41.827  \n",
      "1415300 2022-03-18 15:14:42.120  \n",
      "2318259 2022-03-18 15:14:42.134  \n",
      "952322  2022-03-18 15:14:42.170  \n",
      "2114134 2022-03-18 15:14:42.282  \n",
      "\n",
      "Data info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2500000 entries, 1033131 to 2186369\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Dtype         \n",
      "---  ------        -----         \n",
      " 0   user_id       int64         \n",
      " 1   pratilipi_id  int64         \n",
      " 2   read_percent  float64       \n",
      " 3   updated_at    datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(1), int64(2)\n",
      "memory usage: 95.4 MB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(\"Checking data structure:\")\n",
    "print(\"\\nColumns:\", interactions_df.columns.tolist())\n",
    "print(\"\\nSample of data:\")\n",
    "print(interactions_df.head())\n",
    "print(\"\\nData info:\")\n",
    "print(interactions_df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.sparse import csr_matrix\n",
    "\n",
    "user_mapping = {id: i for i, id in enumerate(\n",
    "    interactions_df['user_id'].unique())}\n",
    "item_mapping = {id: i for i, id in enumerate(\n",
    "    interactions_df['pratilipi_id'].unique())}\n",
    "\n",
    "# 2. Create the necessary variables for sparse matrix\n",
    "user_indices = [user_mapping[user] for user in interactions_df['user_id']]\n",
    "item_indices = [item_mapping[item] for item in interactions_df['pratilipi_id']]\n",
    "# Using read_percent as ratings\n",
    "ratings = interactions_df['read_percent'].values\n",
    "\n",
    "# 3. Get dimensions for the matrix\n",
    "n_users = len(user_mapping)\n",
    "n_items = len(item_mapping)\n",
    "\n",
    "# 4. Now create the sparse matrix\n",
    "sparse_matrix = csr_matrix((ratings, (user_indices, item_indices)),\n",
    "                           shape=(n_users, n_items),\n",
    "                           dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "def split_train_test(sparse_matrix, test_size=0.25, random_state=42):\n",
    "    \"\"\"\n",
    "    Split the sparse interaction matrix into training and test sets\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    sparse_matrix : scipy.sparse.csr_matrix\n",
    "        The user-item interaction matrix\n",
    "    test_size : float\n",
    "        Proportion of data to use for testing (default: 0.25 as per README)\n",
    "    random_state : int\n",
    "        Random seed for reproducibility\n",
    "        \n",
    "    Returns:\n",
    "    --------\n",
    "    train_matrix : scipy.sparse.csr_matrix\n",
    "        Training set matrix\n",
    "    test_matrix : scipy.sparse.csr_matrix\n",
    "        Test set matrix\n",
    "    \"\"\"\n",
    "\n",
    "    # Get indices of all non-zero elements\n",
    "    nonzero = sparse_matrix.nonzero()\n",
    "    indices = list(zip(nonzero[0], nonzero[1]))\n",
    "    values = sparse_matrix.data\n",
    "\n",
    "    # Split the indices\n",
    "    train_indices, test_indices = train_test_split(\n",
    "        range(len(indices)),\n",
    "        test_size=test_size,\n",
    "        random_state=random_state\n",
    "    )\n",
    "\n",
    "    # Create training matrix\n",
    "    train_matrix = sparse_matrix.copy()\n",
    "    train_matrix.data = np.zeros_like(train_matrix.data)\n",
    "    for idx in train_indices:\n",
    "        row, col = indices[idx]\n",
    "        train_matrix[row, col] = values[idx]\n",
    "\n",
    "    # Create test matrix\n",
    "    test_matrix = sparse_matrix.copy()\n",
    "    test_matrix.data = np.zeros_like(test_matrix.data)\n",
    "    for idx in test_indices:\n",
    "        row, col = indices[idx]\n",
    "        test_matrix[row, col] = values[idx]\n",
    "\n",
    "    # Convert to CSR format for efficiency\n",
    "    train_matrix = train_matrix.tocsr()\n",
    "    test_matrix = test_matrix.tocsr()\n",
    "\n",
    "    return train_matrix, test_matrix\n",
    "\n",
    "\n",
    "# Usage:\n",
    "train_matrix, test_matrix = split_train_test(sparse_matrix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original matrix density: 0.000043\n",
      "Training matrix density: 0.000043\n",
      "Test matrix density: 0.000043\n"
     ]
    }
   ],
   "source": [
    "# Print statistics about the split\n",
    "print(\n",
    "    f\"Original matrix density: {sparse_matrix.nnz / (sparse_matrix.shape[0] * sparse_matrix.shape[1]):.6f}\")\n",
    "print(\n",
    "    f\"Training matrix density: {train_matrix.nnz / (train_matrix.shape[0] * train_matrix.shape[1]):.6f}\")\n",
    "print(\n",
    "    f\"Test matrix density: {test_matrix.nnz / (test_matrix.shape[0] * test_matrix.shape[1]):.6f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "class PratilipiDataset(Dataset):\n",
    "    def __init__(self, user_indices, item_indices, ratings):\n",
    "        self.user_indices = torch.LongTensor(user_indices)\n",
    "        self.item_indices = torch.LongTensor(item_indices)\n",
    "        self.ratings = torch.FloatTensor(ratings)\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.ratings)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        return (self.user_indices[idx], self.item_indices[idx], self.ratings[idx])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NCF(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embedding_dim=50):\n",
    "        super(NCF, self).__init__()\n",
    "\n",
    "        # Embedding layers\n",
    "        self.user_embedding = nn.Embedding(num_users, embedding_dim)\n",
    "        self.item_embedding = nn.Embedding(num_items, embedding_dim)\n",
    "\n",
    "        # MLP layers\n",
    "        self.fc_layers = nn.Sequential(\n",
    "            nn.Linear(2 * embedding_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, 64),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(64, 32),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(32, 1),\n",
    "            nn.Sigmoid()\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def forward(self, user_input, item_input):\n",
    "    # Get embeddings\n",
    "    user_embedded = self.user_embedding(user_input)\n",
    "    item_embedded = self.item_embedding(item_input)\n",
    "\n",
    "    # Concatenate embeddings\n",
    "    vector = torch.cat([user_embedded, item_embedded], dim=-1)\n",
    "\n",
    "    # Pass through MLP\n",
    "    prediction = self.fc_layers(vector)\n",
    "    return prediction.squeeze()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model(model, train_loader, epochs=10, lr=0.001, device='cuda'):\n",
    "    model = model.to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for epoch in range(epochs):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        for batch_idx, (users, items, ratings) in enumerate(train_loader):\n",
    "            users = users.to(device)\n",
    "            items = items.to(device)\n",
    "            ratings = ratings.to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            predictions = model(users, items)\n",
    "            loss = criterion(predictions, ratings)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            total_loss += loss.item()\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f'Epoch {epoch+1}/{epochs} - Loss: {avg_loss:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_and_train():\n",
    "    # Assuming you have your sparse_matrix and mappings from previous code\n",
    "    num_users = len(user_to_idx)\n",
    "    num_items = len(item_to_idx)\n",
    "\n",
    "    # Create dataset\n",
    "    dataset = PratilipiDataset(user_indices, item_indices, ratings)\n",
    "    train_loader = DataLoader(dataset, batch_size=1024, shuffle=True)\n",
    "\n",
    "    # Initialize model\n",
    "    model = NCF(num_users, num_items)\n",
    "\n",
    "    # Train model\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    train_model(model, train_loader, device=device)\n",
    "\n",
    "    return model"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
